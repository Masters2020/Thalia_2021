# -*- coding: utf-8 -*-
"""SVM_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tOKKJW7VTd_vugt6DglQGXTGs8llXyDu

#### First run extract_keywords.py before running the code below. 

Important: the NER keyword lists can be tested in exactly the same way as the regular keyword lists. Just change the filepath. You have to run the bert_conll_2003.py and/or bert_gmb.py files first to generate the NER keyword lists.

The code below is based on the work from Raf van den Eijnden (2020).

The code in this file is to test all the combinations of number of keywords and C-parameter values for all four biases. The best combination of hyperparameters derived by tuning can also be checked in the output.
"""

import pickle
from google.colab import drive

drive.mount('/content/drive')

"""After cleaning the raw dataset the data can be loaded with the following code:"""

# open training_data_cleaned & test_data_cleaned without making them first 
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Cleaned data 1'

import os
os.chdir(filepath)
with open('training_data_cleaned.pickle', 'rb') as f: 
  training_data_cleaned = pickle.load(f)
with open('test_data_cleaned.pickle', 'rb') as f: 
  test_data_cleaned = pickle.load(f)

# check
print(len(training_data_cleaned))
print(len(test_data_cleaned))

# we determine some hyperparameters here for the rest of the research, here we can change them to see the influence of certain model parameters
# as wel as set some key variables
keyword_number = [10, 15, 20, 50, 100, 200, 500, 750, 1000]
SVM_C = [0.5, 0.8, 1, 3, 5, 10]

"""The code to load the keyword lists once they are created. """

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 10
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_10 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_10 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 15
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_15 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_15 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 20
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_20 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_20 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 50
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_50 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_50 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 100
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_100 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_100 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 200
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_200 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_200 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 500
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_500 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_500 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 750
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_750 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_750 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 1000
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_1000 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_1000 = pickle.load(f)

"""Transform the cleaned dataset into a vectorized list"""

# Feature extraction
from sklearn.feature_extraction.text import TfidfVectorizer

# for the train set I create X and y
X_trainingdata = []
y_trainingdata = []
for item in training_data_cleaned:
    X_trainingdata.append(item[0])
    y_trainingdata.append(item[1])

# Here I transform X to fit the input of the tfidf vectorizer
list_document_tokens = []
for i, document in enumerate(X_trainingdata):
    list_document_tokens.append(training_data_cleaned[i][0])
tfidf_input = []
for document in list_document_tokens:
    tfidf_input.append(" ".join(document))

# I do the same for my test data
X_testdata = []
y_testdata = []
for item in test_data_cleaned:
    X_testdata.append(item[0])
    y_testdata.append(item[1])

# once again making them fit
list_document_tokens_test = []
for i, document in enumerate(X_testdata):
    list_document_tokens_test.append(test_data_cleaned[i][0])
tfidf_input_test = []
for document in list_document_tokens_test:
    tfidf_input_test.append(" ".join(document))



"""Transformation of the training and test data"""

#for training I fit and transform the train data:
tv_training = TfidfVectorizer(stop_words=None, max_features=10000)
tf_idf_prel_training = tv_training.fit_transform(tfidf_input)
tf_idf_vector_training = tf_idf_prel_training.toarray()
tf_idf_feature_names_training = tv_training.get_feature_names()

#for test, using the same vectorizer I only transform the test data
tf_idf_prel_test = tv_training.transform(tfidf_input_test)
tf_idf_vector_test = tf_idf_prel_test.toarray()

### We route this back in X and Y variables (X_val & y_val for the validation stage, X_ts0 & y_tst for the final test set)
#
X_val = tf_idf_vector_training
y_val = []

# Here we condense class 2 & 3 into one class, leaving is with 0 (non conspiratorial) or 1 (conspiratorial)
for document in y_trainingdata:
    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val.append(class_made)

import numpy as np
y_val = np.array(y_val)

X_tst = tf_idf_vector_test
y_tst = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst.append(class_made)

y_tst = np.array(y_tst)



"""**BIAS 1 - BASE**"""

# we need gensims word2vec here
import gensim
import logging

"""**Keyword = 10**"""

# here I start by calculating our bias multiplier matrix
X_bias1_10 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_10:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_10.append(counter)

# I now have a matrix containing a multiplier score for each word in the TF-IDF Vocabulary, based on similarity to our keyword lists
# now I multiply that matrix with each row of our TF-IDF vectors in both training and test sets
X_val_biased_1_10 = []
for i,matrixrow in enumerate(X_val):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias1_10[j]*item)

    X_val_biased_1_10.append(temp_num_10)
X_val_biased_1_10 = np.array(X_val_biased_1_10)

X_test_biased_1_10 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias1_10[j]*item)

    X_test_biased_1_10.append(temp_num_10)
X_test_biased_1_10 = np.array(X_test_biased_1_10)

# Now I route this back in X and Y variables
#for training:
X_val_bias1_10 = X_val_biased_1_10
y_val_bias1_10 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_10.append(class_made)

y_val_bias1_10 = np.array(y_val_bias1_10)

# for testing
X_tst_bias1_10 = X_test_biased_1_10
y_tst_bias1_10 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_10.append(class_made)

y_tst_bias1_10 = np.array(y_tst_bias1_10)

"""**Keyword = 15**"""

# here I start by calculating our bias multiplier matrix
X_bias1_15 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_15:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_15.append(counter)

# I now have a matrix containing a multiplier score for each word in the TF-IDF Vocabulary, based on similarity to our keyword lists
# now I multiply that matrix with each row of our TF-IDF vectors in both training and test sets
X_val_biased_1_15 = []
for i,matrixrow in enumerate(X_val):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias1_15[j]*item)

    X_val_biased_1_15.append(temp_num_15)
X_val_biased_1_15 = np.array(X_val_biased_1_15)

X_test_biased_1_15 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias1_15[j]*item)

    X_test_biased_1_15.append(temp_num_15)
X_test_biased_1_15 = np.array(X_test_biased_1_15)

# Now I route this back in X and Y variables
#for training:
X_val_bias1_15 = X_val_biased_1_15
y_val_bias1_15 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_15.append(class_made)

y_val_bias1_15 = np.array(y_val_bias1_15)

# for testing
X_tst_bias1_15 = X_test_biased_1_15
y_tst_bias1_15 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_15.append(class_made)

y_tst_bias1_15 = np.array(y_tst_bias1_15)

"""**Keyword = 20**"""

# here I start by calculating our bias multiplier matrix
X_bias1_20 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_20:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_20.append(counter)

# I now have a matrix containing a multiplier score for each word in the TF-IDF Vocabulary, based on similarity to our keyword lists
# now I multiply that matrix with each row of our TF-IDF vectors in both training and test sets
X_val_biased_1_20 = []
for i,matrixrow in enumerate(X_val):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias1_20[j]*item)

    X_val_biased_1_20.append(temp_num_20)
X_val_biased_1_20 = np.array(X_val_biased_1_20)

X_test_biased_1_20 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias1_20[j]*item)

    X_test_biased_1_20.append(temp_num_20)
X_test_biased_1_20 = np.array(X_test_biased_1_20)

# Now I route this back in X and Y variables
#for training:
X_val_bias1_20 = X_val_biased_1_20
y_val_bias1_20 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_20.append(class_made)

y_val_bias1_20 = np.array(y_val_bias1_20)

# for testing
X_tst_bias1_20 = X_test_biased_1_20
y_tst_bias1_20 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_20.append(class_made)

y_tst_bias1_20 = np.array(y_tst_bias1_20)

"""**Keyword = 50**"""

# here I start by calculating our bias multiplier matrix
X_bias1_50 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_50:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_50.append(counter)

X_val_biased_1_50 = []
for i,matrixrow in enumerate(X_val):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias1_50[j]*item)

    X_val_biased_1_50.append(temp_num_50)
X_val_biased_1_50 = np.array(X_val_biased_1_50)

X_test_biased_1_50 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias1_50[j]*item)

    X_test_biased_1_50.append(temp_num_50)
X_test_biased_1_50 = np.array(X_test_biased_1_50)

X_val_bias1_50 = X_val_biased_1_50
y_val_bias1_50 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_50.append(class_made)

y_val_bias1_50 = np.array(y_val_bias1_50)

# for testing
X_tst_bias1_50 = X_test_biased_1_50
y_tst_bias1_50 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_50.append(class_made)

y_tst_bias1_50 = np.array(y_tst_bias1_50)

"""**Keyword = 100**"""

# here I start by calculating our bias multiplier matrix
X_bias1_100 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_100:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_100.append(counter)

X_val_biased_1_100 = []
for i,matrixrow in enumerate(X_val):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias1_100[j]*item)

    X_val_biased_1_100.append(temp_num_100)
X_val_biased_1_100 = np.array(X_val_biased_1_100)

X_test_biased_1_100 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias1_100[j]*item)

    X_test_biased_1_100.append(temp_num_100)
X_test_biased_1_100 = np.array(X_test_biased_1_100)

X_val_bias1_100 = X_val_biased_1_100
y_val_bias1_100 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_100.append(class_made)

y_val_bias1_100 = np.array(y_val_bias1_100)

# for testing
X_tst_bias1_100 = X_test_biased_1_100
y_tst_bias1_100 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_100.append(class_made)

y_tst_bias1_100 = np.array(y_tst_bias1_100)

"""**Keyword = 200**"""

# here I start by calculating our bias multiplier matrix
X_bias1_200 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_200:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_200.append(counter)

X_val_biased_1_200 = []
for i,matrixrow in enumerate(X_val):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias1_200[j]*item)

    X_val_biased_1_200.append(temp_num_200)
X_val_biased_1_200 = np.array(X_val_biased_1_200)

X_test_biased_1_200 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias1_200[j]*item)

    X_test_biased_1_200.append(temp_num_200)
X_test_biased_1_200 = np.array(X_test_biased_1_200)

X_val_bias1_200 = X_val_biased_1_200
y_val_bias1_200 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_200.append(class_made)

y_val_bias1_200 = np.array(y_val_bias1_200)

# for testing
X_tst_bias1_200 = X_test_biased_1_200
y_tst_bias1_200 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_200.append(class_made)

y_tst_bias1_200 = np.array(y_tst_bias1_200)

"""**Keyword = 500**"""

# here I start by calculating our bias multiplier matrix
X_bias1_500 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_500:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_500.append(counter)

X_val_biased_1_500 = []
for i,matrixrow in enumerate(X_val):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias1_500[j]*item)

    X_val_biased_1_500.append(temp_num_500)
X_val_biased_1_500 = np.array(X_val_biased_1_500)

X_test_biased_1_500 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias1_500[j]*item)

    X_test_biased_1_500.append(temp_num_500)
X_test_biased_1_500 = np.array(X_test_biased_1_500)

X_val_bias1_500 = X_val_biased_1_500
y_val_bias1_500 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_500.append(class_made)

y_val_bias1_500 = np.array(y_val_bias1_500)

# for testing
X_tst_bias1_500 = X_test_biased_1_500
y_tst_bias1_500 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_500.append(class_made)

y_tst_bias1_500 = np.array(y_tst_bias1_500)

"""**Keyword = 750**"""

# here I start by calculating our bias multiplier matrix
X_bias1_750 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_750:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_750.append(counter)
  
X_val_biased_1_750 = []
for i,matrixrow in enumerate(X_val):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias1_750[j]*item)

    X_val_biased_1_750.append(temp_num_750)
X_val_biased_1_750 = np.array(X_val_biased_1_750)

X_test_biased_1_750 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias1_750[j]*item)

    X_test_biased_1_750.append(temp_num_750)
X_test_biased_1_750 = np.array(X_test_biased_1_750)

X_val_bias1_750 = X_val_biased_1_750
y_val_bias1_750 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_750.append(class_made)

y_val_bias1_750 = np.array(y_val_bias1_750)

# for testing
X_tst_bias1_750 = X_test_biased_1_750
y_tst_bias1_750 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_750.append(class_made)

y_tst_bias1_750 = np.array(y_tst_bias1_750)

"""**Keyword = 1000**"""

# here I start by calculating our bias multiplier matrix
X_bias1_1000 = []

# initialise and train the Word2Vec model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_1000:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias1_1000.append(counter)

X_val_biased_1_1000 = []
for i,matrixrow in enumerate(X_val):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias1_1000[j]*item)

    X_val_biased_1_1000.append(temp_num_1000)
X_val_biased_1_1000 = np.array(X_val_biased_1_1000)

X_test_biased_1_1000 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias1_1000[j]*item)

    X_test_biased_1_1000.append(temp_num_1000)
X_test_biased_1_1000 = np.array(X_test_biased_1_1000)

X_val_bias1_1000 = X_val_biased_1_1000
y_val_bias1_1000 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias1_1000.append(class_made)

y_val_bias1_1000 = np.array(y_val_bias1_1000)

# for testing
X_tst_bias1_1000 = X_test_biased_1_1000
y_tst_bias1_1000 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias1_1000.append(class_made)

y_tst_bias1_1000 = np.array(y_tst_bias1_1000)

"""## **SVM BIAS 1 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"""

X_validation = X_val_bias1_10, X_val_bias1_15, X_val_bias1_20, X_val_bias1_50, X_val_bias1_100, X_val_bias1_200, X_val_bias1_500, X_val_bias1_750, X_val_bias1_1000
y_validation = y_val_bias1_10, y_val_bias1_15, y_val_bias1_20, y_val_bias1_50, y_val_bias1_100, y_val_bias1_200, y_val_bias1_500, y_val_bias1_750, y_val_bias1_1000

X_tst_bias1 = X_tst_bias1_10, X_tst_bias1_15, X_tst_bias1_20, X_tst_bias1_50, X_tst_bias1_100, X_tst_bias1_200, X_tst_bias1_500, X_tst_bias1_750, X_tst_bias1_1000
y_tst_bias1 = y_tst_bias1_10, y_tst_bias1_15, y_tst_bias1_20, y_tst_bias1_50, y_tst_bias1_100, y_tst_bias1_200, y_tst_bias1_500, y_tst_bias1_750, y_tst_bias1_1000

# train test split for validation
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support

best_performing_fscore, best_performing_c, best_performing_key = 0, None, None
fscore = 0
count = 0

for x, y, z, a in zip(X_validation, y_validation, X_tst_bias1, y_tst_bias1):
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
  for c in SVM_C:
    classifier2 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)
    classifier2.fit(X_train, y_train)

# validation
    print("SVM validation bias1", "SVM_C = ", c, "number = ", count)
    y_pred2 = classifier2.predict(X_test)
    print(confusion_matrix(y_test, y_pred2))
    print()
    print(classification_report(y_test, y_pred2))
    print()
    print(precision_recall_fscore_support(y_test, y_pred2, average='macro'))
    print(precision_recall_fscore_support(y_test, y_pred2, average='macro')[2])
    print()
    print(accuracy_score(y_test, y_pred2))

# testing

    #for z, a in X_tst_bias1, y_tst_bias1:
    print("SVM test bias1", "SVM_C = ", c, "number = ", count)
    y_pred2_test = classifier2.predict(z)
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    from sklearn.metrics import precision_recall_fscore_support
    print(confusion_matrix(a, y_pred2_test))
    print()
    print(classification_report(a, y_pred2_test))
    print()
    print(precision_recall_fscore_support(a, y_pred2_test, average='macro'))
    print(precision_recall_fscore_support(a, y_pred2_test, average='macro')[2])
    fscore = (precision_recall_fscore_support(a, y_pred2_test, average='macro')[2])
    print()
    print(accuracy_score(a, y_pred2_test))

    if best_performing_fscore < fscore: 
      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count
    count+= 1

print()
print("the best performing C parameter is: ", best_performing_c, "\n", "the best performing keyword number is: ", best_performing_key, "\n", 
      "with an f1-score of :", best_performing_fscore, "\n", "for max_feat = 1000")

"""**BIAS 2 - BASE**

**Keyword = 10**
"""

# now to bias the X for bias 2
X_bias2_10 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_10:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_10.append(counter)

X_val_biased_2_10 = []
for i,matrixrow in enumerate(X_val):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias2_10[j]*item)

    X_val_biased_2_10.append(temp_num_10)
X_val_biased_2_10 = np.array(X_val_biased_2_10)

X_test_biased_2_10 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias2_10[j]*item)

    X_test_biased_2_10.append(temp_num_10)
X_test_biased_2_10 = np.array(X_test_biased_2_10)

X_val_bias2_10 = X_val_biased_2_10
y_val_bias2_10 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_10.append(class_made)

y_val_bias2_10 = np.array(y_val_bias2_10)

X_tst_bias2_10 = X_test_biased_2_10
y_tst_bias2_10 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_10.append(class_made)

y_tst_bias2_10 = np.array(y_tst_bias2_10)

"""**Keyword = 15**"""

# now to bias the X for bias 2
X_bias2_15 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_15:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_15.append(counter)

X_val_biased_2_15 = []
for i,matrixrow in enumerate(X_val):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias2_15[j]*item)

    X_val_biased_2_15.append(temp_num_15)
X_val_biased_2_15 = np.array(X_val_biased_2_15)

X_test_biased_2_15 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias2_15[j]*item)

    X_test_biased_2_15.append(temp_num_15)
X_test_biased_2_15 = np.array(X_test_biased_2_15)

X_val_bias2_15 = X_val_biased_2_15
y_val_bias2_15 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_15.append(class_made)

y_val_bias2_15 = np.array(y_val_bias2_15)

X_tst_bias2_15 = X_test_biased_2_15
y_tst_bias2_15 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_15.append(class_made)

y_tst_bias2_15 = np.array(y_tst_bias2_15)

"""**Keyword = 20**"""

# now to bias the X for bias 2
X_bias2_20 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_20:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_20.append(counter)

X_val_biased_2_20 = []
for i,matrixrow in enumerate(X_val):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias2_20[j]*item)

    X_val_biased_2_20.append(temp_num_20)
X_val_biased_2_20 = np.array(X_val_biased_2_20)

X_test_biased_2_20 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias2_20[j]*item)

    X_test_biased_2_20.append(temp_num_20)
X_test_biased_2_20 = np.array(X_test_biased_2_20)

X_val_bias2_20 = X_val_biased_2_20
y_val_bias2_20 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_20.append(class_made)

y_val_bias2_20 = np.array(y_val_bias2_20)

X_tst_bias2_20 = X_test_biased_2_20
y_tst_bias2_20 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_20.append(class_made)

y_tst_bias2_20 = np.array(y_tst_bias2_20)

"""**Keyword = 50**"""

# now to bias the X for bias 2
X_bias2_50 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_50:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_50.append(counter)

X_val_biased_2_50 = []
for i,matrixrow in enumerate(X_val):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias2_50[j]*item)

    X_val_biased_2_50.append(temp_num_50)
X_val_biased_2_50 = np.array(X_val_biased_2_50)

X_test_biased_2_50 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias2_50[j]*item)

    X_test_biased_2_50.append(temp_num_50)
X_test_biased_2_50 = np.array(X_test_biased_2_50)

X_val_bias2_50 = X_val_biased_2_50
y_val_bias2_50 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_50.append(class_made)

y_val_bias2_50 = np.array(y_val_bias2_50)

X_tst_bias2_50 = X_test_biased_2_50
y_tst_bias2_50 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_50.append(class_made)

y_tst_bias2_50 = np.array(y_tst_bias2_50)

"""**Keyword = 100**"""

# now to bias the X for bias 2
X_bias2_100 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_100:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_100.append(counter)

X_val_biased_2_100 = []
for i,matrixrow in enumerate(X_val):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias2_100[j]*item)

    X_val_biased_2_100.append(temp_num_100)
X_val_biased_2_100 = np.array(X_val_biased_2_100)

X_test_biased_2_100 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias2_100[j]*item)

    X_test_biased_2_100.append(temp_num_100)
X_test_biased_2_100 = np.array(X_test_biased_2_100)

X_val_bias2_100 = X_val_biased_2_100
y_val_bias2_100 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_100.append(class_made)

y_val_bias2_100 = np.array(y_val_bias2_100)

X_tst_bias2_100 = X_test_biased_2_100
y_tst_bias2_100 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_100.append(class_made)

y_tst_bias2_100 = np.array(y_tst_bias2_100)

"""**Keyword = 200**"""

# now to bias the X for bias 2
X_bias2_200 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_200:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_200.append(counter)

X_val_biased_2_200 = []
for i,matrixrow in enumerate(X_val):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias2_200[j]*item)

    X_val_biased_2_200.append(temp_num_200)
X_val_biased_2_200 = np.array(X_val_biased_2_200)

X_test_biased_2_200 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias2_200[j]*item)

    X_test_biased_2_200.append(temp_num_200)
X_test_biased_2_200 = np.array(X_test_biased_2_200)

X_val_bias2_200 = X_val_biased_2_200
y_val_bias2_200 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_200.append(class_made)

y_val_bias2_200 = np.array(y_val_bias2_200)

X_tst_bias2_200 = X_test_biased_2_200
y_tst_bias2_200 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_200.append(class_made)

y_tst_bias2_200 = np.array(y_tst_bias2_200)

"""**Keyword = 500**"""

# now to bias the X for bias 2
X_bias2_500 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_500:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_500.append(counter)

X_val_biased_2_500 = []
for i,matrixrow in enumerate(X_val):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias2_500[j]*item)

    X_val_biased_2_500.append(temp_num_500)
X_val_biased_2_500 = np.array(X_val_biased_2_500)

X_test_biased_2_500 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias2_500[j]*item)

    X_test_biased_2_500.append(temp_num_500)
X_test_biased_2_500 = np.array(X_test_biased_2_500)

X_val_bias2_500 = X_val_biased_2_500
y_val_bias2_500 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_500.append(class_made)

y_val_bias2_500 = np.array(y_val_bias2_500)

X_tst_bias2_500 = X_test_biased_2_500
y_tst_bias2_500 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_500.append(class_made)

y_tst_bias2_500 = np.array(y_tst_bias2_500)

"""**Keyword = 750**"""

# now to bias the X for bias 2
X_bias2_750 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_750:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_750.append(counter)

X_val_biased_2_750 = []
for i,matrixrow in enumerate(X_val):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias2_750[j]*item)

    X_val_biased_2_750.append(temp_num_750)
X_val_biased_2_750 = np.array(X_val_biased_2_750)

X_test_biased_2_750 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias2_750[j]*item)

    X_test_biased_2_750.append(temp_num_750)
X_test_biased_2_750 = np.array(X_test_biased_2_750)

X_val_bias2_750 = X_val_biased_2_750
y_val_bias2_750 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_750.append(class_made)

y_val_bias2_750 = np.array(y_val_bias2_750)

X_tst_bias2_750 = X_test_biased_2_750
y_tst_bias2_750 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_750.append(class_made)

y_tst_bias2_750 = np.array(y_tst_bias2_750)

"""**Keyword = 1000**"""

# now to bias the X for bias 2
X_bias2_1000 = []

# initialise and train the model
model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)

# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_1000:
            try:
                counter += model.wv.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
          counter = counter/averager
        except:
          counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias2_1000.append(counter)

X_val_biased_2_1000 = []
for i,matrixrow in enumerate(X_val):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias2_1000[j]*item)

    X_val_biased_2_1000.append(temp_num_1000)
X_val_biased_2_1000 = np.array(X_val_biased_2_1000)

X_test_biased_2_1000 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias2_1000[j]*item)

    X_test_biased_2_1000.append(temp_num_1000)
X_test_biased_2_1000 = np.array(X_test_biased_2_1000)

X_val_bias2_1000 = X_val_biased_2_1000
y_val_bias2_1000 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias2_1000.append(class_made)

y_val_bias2_1000 = np.array(y_val_bias2_1000)

X_tst_bias2_1000 = X_test_biased_2_1000
y_tst_bias2_1000 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias2_1000.append(class_made)

y_tst_bias2_1000 = np.array(y_tst_bias2_1000)

"""## **SVM BIAS 2 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"""

X_validation = X_val_bias2_10, X_val_bias2_15, X_val_bias2_20, X_val_bias2_50, X_val_bias2_100, X_val_bias2_200, X_val_bias2_500, X_val_bias2_750, X_val_bias2_1000
y_validation = y_val_bias2_10, y_val_bias2_15, y_val_bias2_20, y_val_bias2_50, y_val_bias2_100, y_val_bias2_200, y_val_bias2_500, y_val_bias2_750, y_val_bias2_1000

X_tst_bias2 = X_tst_bias2_10, X_tst_bias2_15, X_tst_bias2_20, X_tst_bias2_50, X_tst_bias2_100, X_tst_bias2_200, X_tst_bias2_500, X_tst_bias2_750, X_tst_bias2_1000
y_tst_bias2 = y_tst_bias2_10, y_tst_bias2_15, y_tst_bias2_20, y_tst_bias2_50, y_tst_bias2_100, y_tst_bias2_200, y_tst_bias2_500, y_tst_bias2_750, y_tst_bias2_1000

# train test split for validation
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support

best_performing_fscore, best_performing_c, best_performing_key = 0, None, None
fscore = 0
count = 0

for x, y, z, a in zip(X_validation, y_validation, X_tst_bias2, y_tst_bias2):
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
  for c in SVM_C:
    classifier3 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)
    classifier3.fit(X_train, y_train)

# validation
    print("SVM validation bias2", "SVM_C = ", c, "number = ", count)
    y_pred3 = classifier3.predict(X_test)
    print(confusion_matrix(y_test, y_pred3))
    print()
    print(classification_report(y_test, y_pred3))
    print()
    print(precision_recall_fscore_support(y_test, y_pred3, average='macro'))
    print(precision_recall_fscore_support(y_test, y_pred3, average='macro')[2])
    print()
    print(accuracy_score(y_test, y_pred3))

# testing

    #for z, a in X_tst_bias1, y_tst_bias1:
    print("SVM test bias2", "SVM_C = ", c, "number = ", count)
    y_pred3_test = classifier3.predict(z)
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    from sklearn.metrics import precision_recall_fscore_support
    print(confusion_matrix(a, y_pred3_test))
    print()
    print(classification_report(a, y_pred3_test))
    print()
    print(precision_recall_fscore_support(a, y_pred3_test, average='macro'))
    print(precision_recall_fscore_support(a, y_pred3_test, average='macro')[2])
    fscore = (precision_recall_fscore_support(a, y_pred3_test, average='macro')[2])
    print()
    print(accuracy_score(a, y_pred3_test))

    if best_performing_fscore < fscore: 
      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count
    count+= 1

print()
print("the best performing C parameter is: ", best_performing_c, "\n", "the best performing keyword number is: ", best_performing_key, "\n", 
      "with an f1-score of :", best_performing_fscore, "\n", "for max_feat = 1000")



"""**BIAS 3 - BASE**"""

from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

# load the pretrained file GLove for word similairty
glove_file = datapath('/content/drive/My Drive/Colab Notebooks/Thesis Project/Datasets GloVe/glove.6B.100d.txt')
word2vec_glove_file = get_tmpfile('glove.6B.100d.txt')

glove2word2vec(glove_file,word2vec_glove_file)

"""**Keyword = 10**"""

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# create biases in the same fashion as before
X_bias3_10 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_10:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_10.append(counter)

X_val_biased_3_10 = []
for i,matrixrow in enumerate(X_val):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias3_10[j]*item)

    X_val_biased_3_10.append(temp_num_10)
X_val_biased_3_10 = np.array(X_val_biased_3_10)

X_test_biased_3_10 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias3_10[j]*item)

    X_test_biased_3_10.append(temp_num_10)
X_test_biased_3_10 = np.array(X_test_biased_3_10)

X_val_bias3_10 = X_val_biased_3_10
y_val_bias3_10 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_10.append(class_made)

y_val_bias3_10 = np.array(y_val_bias3_10)

X_tst_bias3_10 = X_test_biased_3_10
y_tst_bias3_10 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_10.append(class_made)

y_tst_bias3_10 = np.array(y_tst_bias3_10)

"""**Keyword = 15**"""

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# create biases in the same fashion as before
X_bias3_15 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_15:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_15.append(counter)

X_val_biased_3_15 = []
for i,matrixrow in enumerate(X_val):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias3_15[j]*item)

    X_val_biased_3_15.append(temp_num_15)
X_val_biased_3_15 = np.array(X_val_biased_3_15)

X_test_biased_3_15 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias3_15[j]*item)

    X_test_biased_3_15.append(temp_num_15)
X_test_biased_3_15 = np.array(X_test_biased_3_15)

X_val_bias3_15 = X_val_biased_3_15
y_val_bias3_15 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_15.append(class_made)

y_val_bias3_15 = np.array(y_val_bias3_15)

X_tst_bias3_15 = X_test_biased_3_15
y_tst_bias3_15 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_15.append(class_made)

y_tst_bias3_15 = np.array(y_tst_bias3_15)

"""**Keyword = 20**"""

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# create biases in the same fashion as before
X_bias3_20 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_20:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_20.append(counter)

X_val_biased_3_20 = []
for i,matrixrow in enumerate(X_val):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias3_20[j]*item)

    X_val_biased_3_20.append(temp_num_20)
X_val_biased_3_20 = np.array(X_val_biased_3_20)

X_test_biased_3_20 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias3_20[j]*item)

    X_test_biased_3_20.append(temp_num_20)
X_test_biased_3_20 = np.array(X_test_biased_3_20)

X_val_bias3_20 = X_val_biased_3_20
y_val_bias3_20 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_20.append(class_made)

y_val_bias3_20 = np.array(y_val_bias3_20)

X_tst_bias3_20 = X_test_biased_3_20
y_tst_bias3_20 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_20.append(class_made)

y_tst_bias3_20 = np.array(y_tst_bias3_20)

"""**Keyword = 50**"""

# create biases in the same fashion as before
X_bias3_50 = []

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_50:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_50.append(counter)

X_val_biased_3_50 = []
for i,matrixrow in enumerate(X_val):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias3_50[j]*item)

    X_val_biased_3_50.append(temp_num_50)
X_val_biased_3_50 = np.array(X_val_biased_3_50)

X_test_biased_3_50 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias3_50[j]*item)

    X_test_biased_3_50.append(temp_num_50)
X_test_biased_3_50 = np.array(X_test_biased_3_50)

X_val_bias3_50 = X_val_biased_3_50
y_val_bias3_50 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_50.append(class_made)

y_val_bias3_50 = np.array(y_val_bias3_50)

X_tst_bias3_50 = X_test_biased_3_50
y_tst_bias3_50 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_50.append(class_made)

y_tst_bias3_50 = np.array(y_tst_bias3_50)

"""**Keyword = 100**"""

# create biases in the same fashion as before
X_bias3_100 = []

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_100:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_100.append(counter)

X_val_biased_3_100 = []
for i,matrixrow in enumerate(X_val):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias3_100[j]*item)

    X_val_biased_3_100.append(temp_num_100)
X_val_biased_3_100 = np.array(X_val_biased_3_100)

X_test_biased_3_100 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias3_100[j]*item)

    X_test_biased_3_100.append(temp_num_100)
X_test_biased_3_100 = np.array(X_test_biased_3_100)

X_val_bias3_100 = X_val_biased_3_100
y_val_bias3_100 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_100.append(class_made)

y_val_bias3_100 = np.array(y_val_bias3_100)

X_tst_bias3_100 = X_test_biased_3_100
y_tst_bias3_100 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_100.append(class_made)

y_tst_bias3_100 = np.array(y_tst_bias3_100)

"""**Keyword = 200**"""

# create biases in the same fashion as before
X_bias3_200 = []

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_200:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_200.append(counter)

X_val_biased_3_200 = []
for i,matrixrow in enumerate(X_val):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias3_200[j]*item)

    X_val_biased_3_200.append(temp_num_200)
X_val_biased_3_200 = np.array(X_val_biased_3_200)

X_test_biased_3_200 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias3_200[j]*item)

    X_test_biased_3_200.append(temp_num_200)
X_test_biased_3_200 = np.array(X_test_biased_3_200)

X_val_bias3_200 = X_val_biased_3_200
y_val_bias3_200 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_200.append(class_made)

y_val_bias3_200 = np.array(y_val_bias3_200)

X_tst_bias3_200 = X_test_biased_3_200
y_tst_bias3_200 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_200.append(class_made)

y_tst_bias3_200 = np.array(y_tst_bias3_200)

"""**Keyword = 500**"""

# create biases in the same fashion as before
X_bias3_500 = []

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_500:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_500.append(counter)

X_val_biased_3_500 = []
for i,matrixrow in enumerate(X_val):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias3_500[j]*item)

    X_val_biased_3_500.append(temp_num_500)
X_val_biased_3_500 = np.array(X_val_biased_3_500)

X_test_biased_3_500 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias3_500[j]*item)

    X_test_biased_3_500.append(temp_num_500)
X_test_biased_3_500 = np.array(X_test_biased_3_500)

X_val_bias3_500 = X_val_biased_3_500
y_val_bias3_500 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_500.append(class_made)

y_val_bias3_500 = np.array(y_val_bias3_500)

X_tst_bias3_500 = X_test_biased_3_500
y_tst_bias3_500 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_500.append(class_made)

y_tst_bias3_500 = np.array(y_tst_bias3_500)

"""**Keyword = 750**"""

# create biases in the same fashion as before
X_bias3_750 = []

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_750:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_750.append(counter)

X_val_biased_3_750 = []
for i,matrixrow in enumerate(X_val):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias3_750[j]*item)

    X_val_biased_3_750.append(temp_num_750)
X_val_biased_3_750 = np.array(X_val_biased_3_750)

X_test_biased_3_750 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias3_750[j]*item)

    X_test_biased_3_750.append(temp_num_750)
X_test_biased_3_750 = np.array(X_test_biased_3_750)

X_val_bias3_750 = X_val_biased_3_750
y_val_bias3_750 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_750.append(class_made)

y_val_bias3_750 = np.array(y_val_bias3_750)

X_tst_bias3_750 = X_test_biased_3_750
y_tst_bias3_750 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_750.append(class_made)

y_tst_bias3_750 = np.array(y_tst_bias3_750)

"""**Keyword = 1000**"""

# create biases in the same fashion as before
X_bias3_1000 = []

# initialise and train the model
model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list1_1000:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass

        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias3_1000.append(counter)

X_val_biased_3_1000 = []
for i,matrixrow in enumerate(X_val):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias3_1000[j]*item)

    X_val_biased_3_1000.append(temp_num_1000)
X_val_biased_3_1000 = np.array(X_val_biased_3_1000)

X_test_biased_3_1000 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias3_1000[j]*item)

    X_test_biased_3_1000.append(temp_num_1000)
X_test_biased_3_1000 = np.array(X_test_biased_3_1000)

X_val_bias3_1000 = X_val_biased_3_1000
y_val_bias3_1000 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias3_1000.append(class_made)

y_val_bias3_1000 = np.array(y_val_bias3_1000)

X_tst_bias3_1000 = X_test_biased_3_1000
y_tst_bias3_1000 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias3_1000.append(class_made)

y_tst_bias3_1000 = np.array(y_tst_bias3_1000)

"""## **SVM BIAS 3 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"""

X_validation = X_val_bias3_10, X_val_bias3_15, X_val_bias3_20, X_val_bias3_50, X_val_bias3_100, X_val_bias3_200, X_val_bias3_500, X_val_bias3_750, X_val_bias3_1000
y_validation = y_val_bias3_10, y_val_bias3_15, y_val_bias3_20, y_val_bias3_50, y_val_bias3_100, y_val_bias3_200, y_val_bias3_500, y_val_bias3_750, y_val_bias3_1000

X_tst_bias3 = X_tst_bias3_10, X_tst_bias3_15, X_tst_bias3_20, X_tst_bias3_50, X_tst_bias3_100, X_tst_bias3_200, X_tst_bias3_500, X_tst_bias3_750, X_tst_bias3_1000
y_tst_bias3 = y_tst_bias3_10, y_tst_bias3_15, y_tst_bias3_20, y_tst_bias3_50, y_tst_bias3_100, y_tst_bias3_200, y_tst_bias3_500, y_tst_bias3_750, y_tst_bias3_1000

# train test split for validation
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

best_performing_fscore, best_performing_c, best_performing_key = 0, None, None
fscore = 0
count = 0

for x, y, z, a in zip(X_validation, y_validation, X_tst_bias3, y_tst_bias3):
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
  for c in SVM_C:
    classifier4 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)
    classifier4.fit(X_train, y_train)

# validation
    print("SVM validation bias3", "SVM_C = ", c, "number = ", count)
    y_pred4 = classifier4.predict(X_test)
    print(confusion_matrix(y_test, y_pred4))
    print()
    print(classification_report(y_test, y_pred4))
    print()
    print(precision_recall_fscore_support(y_test, y_pred4, average='macro'))
    print(precision_recall_fscore_support(y_test, y_pred4, average='macro')[2])
    print()
    print(accuracy_score(y_test, y_pred4))

# testing

    #for z, a in X_tst_bias1, y_tst_bias1:
    print("SVM test bias3", "SVM_C = ", c, "number = ", count)
    y_pred4_test = classifier4.predict(z)
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    from sklearn.metrics import precision_recall_fscore_support
    print(confusion_matrix(a, y_pred4_test))
    print()
    print(classification_report(a, y_pred4_test))
    print()
    print(precision_recall_fscore_support(a, y_pred4_test, average='macro'))
    print(precision_recall_fscore_support(a, y_pred4_test, average='macro')[2])
    fscore = (precision_recall_fscore_support(a, y_pred4_test, average='macro')[2])
    print()
    print(accuracy_score(a, y_pred4_test))

    if best_performing_fscore < fscore: 
      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count
    count+= 1

print()
print("the best performing C parameter is: ", best_performing_c, "\n", "the best performing keyword number is: ", best_performing_key, "\n", 
      "with an f1-score of :", best_performing_fscore, "\n", "for max_feat = 4000")



"""**BIAS 4 - BASE**

**Keyword = 10**
"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_10 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_10:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_10.append(counter)

X_val_biased_4_10 = []
for i,matrixrow in enumerate(X_val):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias4_10[j]*item)

    X_val_biased_4_10.append(temp_num_10)
X_val_biased_4_10 = np.array(X_val_biased_4_10)

X_test_biased_4_10 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_10 = []
    for j, item in enumerate(matrixrow):

        temp_num_10.append(X_bias4_10[j]*item)

    X_test_biased_4_10.append(temp_num_10)
X_test_biased_4_10 = np.array(X_test_biased_4_10)

X_val_bias4_10 = X_val_biased_4_10
y_val_bias4_10 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_10.append(class_made)

y_val_bias4_10 = np.array(y_val_bias4_10)

X_tst_bias4_10 = X_test_biased_4_10
y_tst_bias4_10 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_10.append(class_made)

y_tst_bias4_10 = np.array(y_tst_bias4_10)

"""**Keyword = 15**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_15 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_15:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_15.append(counter)

X_val_biased_4_15 = []
for i,matrixrow in enumerate(X_val):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias4_15[j]*item)

    X_val_biased_4_15.append(temp_num_15)
X_val_biased_4_15 = np.array(X_val_biased_4_15)

X_test_biased_4_15 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_15 = []
    for j, item in enumerate(matrixrow):

        temp_num_15.append(X_bias4_15[j]*item)

    X_test_biased_4_15.append(temp_num_15)
X_test_biased_4_15 = np.array(X_test_biased_4_15)

X_val_bias4_15 = X_val_biased_4_15
y_val_bias4_15 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_15.append(class_made)

y_val_bias4_15 = np.array(y_val_bias4_15)

X_tst_bias4_15 = X_test_biased_4_15
y_tst_bias4_15 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_15.append(class_made)

y_tst_bias4_15 = np.array(y_tst_bias4_15)

"""**Keyword = 20**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_20 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_20:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_20.append(counter)

X_val_biased_4_20 = []
for i,matrixrow in enumerate(X_val):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias4_20[j]*item)

    X_val_biased_4_20.append(temp_num_20)
X_val_biased_4_20 = np.array(X_val_biased_4_20)

X_test_biased_4_20 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_20 = []
    for j, item in enumerate(matrixrow):

        temp_num_20.append(X_bias4_20[j]*item)

    X_test_biased_4_20.append(temp_num_20)
X_test_biased_4_20 = np.array(X_test_biased_4_20)

X_val_bias4_20 = X_val_biased_4_20
y_val_bias4_20 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_20.append(class_made)

y_val_bias4_20 = np.array(y_val_bias4_20)

X_tst_bias4_20 = X_test_biased_4_20
y_tst_bias4_20 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_20.append(class_made)

y_tst_bias4_20 = np.array(y_tst_bias4_20)

"""**Keyword = 50**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_50 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_50:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_50.append(counter)

X_val_biased_4_50 = []
for i,matrixrow in enumerate(X_val):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias4_50[j]*item)

    X_val_biased_4_50.append(temp_num_50)
X_val_biased_4_50 = np.array(X_val_biased_4_50)

X_test_biased_4_50 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_50 = []
    for j, item in enumerate(matrixrow):

        temp_num_50.append(X_bias4_50[j]*item)

    X_test_biased_4_50.append(temp_num_50)
X_test_biased_4_50 = np.array(X_test_biased_4_50)

X_val_bias4_50 = X_val_biased_4_50
y_val_bias4_50 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_50.append(class_made)

y_val_bias4_50 = np.array(y_val_bias4_50)

X_tst_bias4_50 = X_test_biased_4_50
y_tst_bias4_50 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_50.append(class_made)

y_tst_bias4_50 = np.array(y_tst_bias4_50)

"""**Keyword = 100**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_100 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_100:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_100.append(counter)

X_val_biased_4_100 = []
for i,matrixrow in enumerate(X_val):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias4_100[j]*item)

    X_val_biased_4_100.append(temp_num_100)
X_val_biased_4_100 = np.array(X_val_biased_4_100)

X_test_biased_4_100 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_100 = []
    for j, item in enumerate(matrixrow):

        temp_num_100.append(X_bias4_100[j]*item)

    X_test_biased_4_100.append(temp_num_100)
X_test_biased_4_100 = np.array(X_test_biased_4_100)

X_val_bias4_100 = X_val_biased_4_100
y_val_bias4_100 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_100.append(class_made)

y_val_bias4_100 = np.array(y_val_bias4_100)

X_tst_bias4_100 = X_test_biased_4_100
y_tst_bias4_100 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_100.append(class_made)

y_tst_bias4_100 = np.array(y_tst_bias4_100)

"""**Keyword = 200**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_200 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_200:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_200.append(counter)

X_val_biased_4_200 = []
for i,matrixrow in enumerate(X_val):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias4_200[j]*item)

    X_val_biased_4_200.append(temp_num_200)
X_val_biased_4_200 = np.array(X_val_biased_4_200)

X_test_biased_4_200 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_200 = []
    for j, item in enumerate(matrixrow):

        temp_num_200.append(X_bias4_200[j]*item)

    X_test_biased_4_200.append(temp_num_200)
X_test_biased_4_200 = np.array(X_test_biased_4_200)

X_val_bias4_200 = X_val_biased_4_200
y_val_bias4_200 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_200.append(class_made)

y_val_bias4_200 = np.array(y_val_bias4_200)

X_tst_bias4_200 = X_test_biased_4_200
y_tst_bias4_200 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_200.append(class_made)

y_tst_bias4_200 = np.array(y_tst_bias4_200)

"""**Keyword = 500**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_500 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_500:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_500.append(counter)

X_val_biased_4_500 = []
for i,matrixrow in enumerate(X_val):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias4_500[j]*item)

    X_val_biased_4_500.append(temp_num_500)
X_val_biased_4_500 = np.array(X_val_biased_4_500)

X_test_biased_4_500 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_500 = []
    for j, item in enumerate(matrixrow):

        temp_num_500.append(X_bias4_500[j]*item)

    X_test_biased_4_500.append(temp_num_500)
X_test_biased_4_500 = np.array(X_test_biased_4_500)

X_val_bias4_500 = X_val_biased_4_500
y_val_bias4_500 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_500.append(class_made)

y_val_bias4_500 = np.array(y_val_bias4_500)

X_tst_bias4_500 = X_test_biased_4_500
y_tst_bias4_500 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_500.append(class_made)

y_tst_bias4_500 = np.array(y_tst_bias4_500)

"""**Keyword = 750**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_750 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_750:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_750.append(counter)

X_val_biased_4_750 = []
for i,matrixrow in enumerate(X_val):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias4_750[j]*item)

    X_val_biased_4_750.append(temp_num_750)
X_val_biased_4_750 = np.array(X_val_biased_4_750)

X_test_biased_4_750 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_750 = []
    for j, item in enumerate(matrixrow):

        temp_num_750.append(X_bias4_750[j]*item)

    X_test_biased_4_750.append(temp_num_750)
X_test_biased_4_750 = np.array(X_test_biased_4_750)

X_val_bias4_750 = X_val_biased_4_750
y_val_bias4_750 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_750.append(class_made)

y_val_bias4_750 = np.array(y_val_bias4_750)

X_tst_bias4_750 = X_test_biased_4_750
y_tst_bias4_750 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_750.append(class_made)

y_tst_bias4_750 = np.array(y_tst_bias4_750)

"""**Keyword = 1000**"""

model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)

# now for the last bias
X_bias4_1000 = []
for w1 in tf_idf_feature_names_training:
        counter = 0
        averager = 0
        for w2 in keyword_list2_1000:
            try:
                counter += model2.similarity(w1, w2)
                averager += 1
            except:
                averager -= 1
                pass


        try:
            counter = counter/averager
        except:
            counter = counter
        if counter > 0:
            counter += 1
        else:
            counter = counter
        X_bias4_1000.append(counter)

X_val_biased_4_1000 = []
for i,matrixrow in enumerate(X_val):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias4_1000[j]*item)

    X_val_biased_4_1000.append(temp_num_1000)
X_val_biased_4_1000 = np.array(X_val_biased_4_1000)

X_test_biased_4_1000 = []
for i,matrixrow in enumerate(X_tst):
    temp_num_1000 = []
    for j, item in enumerate(matrixrow):

        temp_num_1000.append(X_bias4_1000[j]*item)

    X_test_biased_4_1000.append(temp_num_1000)
X_test_biased_4_1000 = np.array(X_test_biased_4_1000)

X_val_bias4_1000 = X_val_biased_4_1000
y_val_bias4_1000 = []
for document in y_trainingdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_val_bias4_1000.append(class_made)

y_val_bias4_1000 = np.array(y_val_bias4_1000)

X_tst_bias4_1000 = X_test_biased_4_1000
y_tst_bias4_1000 = []
for document in y_testdata:

    class_made = 0
    if document == 1:
        class_made = 0
    else:
        class_made = 1
    y_tst_bias4_1000.append(class_made)

y_tst_bias4_1000 = np.array(y_tst_bias4_1000)

"""## **SVM BIAS 4 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"""

X_validation = X_val_bias4_10, X_val_bias4_15, X_val_bias4_20, X_val_bias4_50, X_val_bias4_100, X_val_bias4_200, X_val_bias4_500, X_val_bias4_750, X_val_bias4_1000
y_validation = y_val_bias4_10, y_val_bias4_15, y_val_bias4_20, y_val_bias4_50, y_val_bias4_100, y_val_bias4_200, y_val_bias4_500, y_val_bias4_750, y_val_bias4_1000

X_tst_bias4 = X_tst_bias4_10, X_tst_bias4_15, X_tst_bias4_20, X_tst_bias4_50, X_tst_bias4_100, X_tst_bias4_200, X_tst_bias4_500, X_tst_bias4_750, X_tst_bias4_1000
y_tst_bias4 = y_tst_bias4_10, y_tst_bias4_15, y_tst_bias4_20, y_tst_bias4_50, y_tst_bias4_100, y_tst_bias4_200, y_tst_bias4_500, y_tst_bias4_750, y_tst_bias4_1000

# train test split for validation
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

best_performing_fscore, best_performing_c, best_performing_key = 0, None, None
fscore = 0
count = 0

for x, y, z, a in zip(X_validation, y_validation, X_tst_bias4, y_tst_bias4):
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
  for c in SVM_C:
    classifier6 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)
    classifier6.fit(X_train, y_train)

# validation
    print("SVM validation bias4", "SVM_C = ", c, "number = ", count)
    y_pred5 = classifier6.predict(X_test)
    print(confusion_matrix(y_test, y_pred5))
    print()
    print(classification_report(y_test, y_pred5))
    print()
    print(precision_recall_fscore_support(y_test, y_pred5, average='macro'))
    print(precision_recall_fscore_support(y_test, y_pred5, average='macro')[2])
    print()
    print(accuracy_score(y_test, y_pred5))

# testing

    #for z, a in X_tst_bias1, y_tst_bias1:
    print("SVM test bias4", "SVM_C = ", c, "number = ", count)
    y_pred5_test = classifier6.predict(z)
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    from sklearn.metrics import precision_recall_fscore_support
    print(confusion_matrix(a, y_pred5_test))
    print()
    print(classification_report(a, y_pred5_test))
    print()
    print(precision_recall_fscore_support(a, y_pred5_test, average='macro'))
    print(precision_recall_fscore_support(a, y_pred5_test, average='macro')[2])
    fscore = (precision_recall_fscore_support(a, y_pred5_test, average='macro')[2])
    print()
    print(accuracy_score(a, y_pred5_test))

    if best_performing_fscore < fscore: 
      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count
    count+= 1

print()
print("the best performing C parameter is: ", best_performing_c, "\n", "the best performing keyword number is: ", best_performing_key, "\n", 
      "with an f1-score of :", best_performing_fscore, "\n", "for max_feat = 4000")

