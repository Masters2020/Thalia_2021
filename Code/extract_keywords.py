# -*- coding: utf-8 -*-
"""extract_keywords.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tns3TnYsFPrikRfia9j-yr-8bSjK8wEE
"""

import pickle
from google.colab import drive

drive.mount('/content/drive')

# load training.pkl
path = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Datasets YouTube/training_data.pickle'
with open(path, 'rb') as f:
  training = pickle.loads(f.read())

# load test.pkl
path = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Datasets YouTube/test_data.pickle'
with open(path, 'rb') as f:
  test = pickle.loads(f.read())

# load the nlp moduele
import en_core_web_sm

nlp = en_core_web_sm.load(disable=["parser", "tagger", "ner"])

# do lower-case and remove numbers
def my_cleaner3(text,nlp):

    return[token.lemma_.lower() for token in nlp(text) if not (token.is_alpha==False or len(token.lemma_) <3) ]

def cleaner(training_data,nlp,filepath,filename):
    '''
    Code by Raf van den Eijnden (2020)

    :param training_data: input here your list containing training or test dataset
    :param nlp: specify variable that holds nlp model
    :param filepath: input here your folder where you want the pickle
    :param filepath: input here your name for the pickle

    :return: returns cleaned version (tokenized and lowercased)
    '''

    training_data_cleaned = []
    for i, transcript in enumerate(training_data):
        doc = nlp(training_data[i][0])
        #print("doc: ", doc)
        cleaned_tokens1 = my_cleaner3(doc.text, nlp=nlp)
        #print("cleaned_tokens1: ", cleaned_tokens1)
        training_data_cleaned.append([cleaned_tokens1, training_data[i][1]])
        #print("training_data_cleaned: ", training_data_cleaned)

    import os
    os.chdir(filepath)
    import pickle
    with open(filename+'.pickle', 'wb') as f:
        pickle.dump(training_data_cleaned, f)
    return training_data_cleaned

# clean all data and pickle them
training_data_cleaned = cleaner(training, nlp,'/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Cleaned data 1', 'training_data_cleaned')
test_data_cleaned = cleaner(test, nlp,'/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Cleaned data 1', 'test_data_cleaned')



"""After cleaning the raw dataset the data can be loaded with the following code:"""

# open training_data_cleaned & test_data_cleaned without making them first 
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Cleaned data 1'

import os
os.chdir(filepath)
with open('training_data_cleaned.pickle', 'rb') as f: 
  training_data_cleaned = pickle.load(f)
with open('test_data_cleaned.pickle', 'rb') as f: 
  test_data_cleaned = pickle.load(f)

# check
print(len(training_data_cleaned))
print(len(test_data_cleaned))

# we determine some hyperparameters here for the rest of the research, here we can change them to see the influence of certain model parameters
# as wel as set some key variables
keyword_number = [10, 15, 20, 50, 100, 200, 500, 750, 1000]
SVM_C = [0.5, 0.8, 1, 3, 5, 10]

def keywordextractor(filepath_trans, filepath_togo, filename,keywords_num):

    """ 
    Code by Raf van den Eijnden (2020)
    
    this function creates two picklefiles, each has a keyword list
    first parameter; filepath = the path where you want the pickles to go, and the path where your transcript data is
    filename: name of the pickle: for example; 'cleaned_transcript_data.pickle'
    second parameter; keywords_num = amount of keywords you want in your list"""

    import os
    import pickle
    import numpy as np

    os.chdir(filepath_trans)
    with open(filename, 'rb') as f:
        transcripts_cleaned = pickle.load(f)


    keywords_number = keywords_num
    ### taking only transcripts
    list_document_tokens = []
    for i, document in enumerate(transcripts_cleaned):
        list_document_tokens.append(transcripts_cleaned[i][0])

    # create a list of documents to input to tfidfvectorizer
    tfidf_input = []
    for document in list_document_tokens:
        tfidf_input.append(" ".join(document))

    ### split it by class
    list_document_tokens_consp = []
    list_document_tokens_nonconsp = []
    for i, document in enumerate(transcripts_cleaned):
      #print("i: ", i)
      #print("document: ", document)
      if document[1] == '1':

          list_document_tokens_nonconsp.append(transcripts_cleaned[i][0])
      else:
          list_document_tokens_consp.append(transcripts_cleaned[i][0])

    tfidf_input_consp = []
    tfidf_input_nonconsp = []
    for document in list_document_tokens_consp:
        tfidf_input_consp.append(" ".join(document))
    for document in list_document_tokens_nonconsp:
        tfidf_input_nonconsp.append(" ".join(document))

    # now for keyword extraction method 1

    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.feature_extraction.text import CountVectorizer

    tv = TfidfVectorizer(stop_words=None, max_features=10000)

    vocab = tv.fit(tfidf_input)
    feature_names = vocab.get_feature_names()

    word_count_vector = tv.fit_transform(tfidf_input).toarray()

    word_count_vector_transposed = word_count_vector.T

    total_word_info = []
    for i, word in enumerate(word_count_vector_transposed):
        tempword = word.tolist()
        word_info = []

        for j, document in enumerate(tempword):
            word_info.append([j, document, transcripts_cleaned[j][1]])
        total_word_info.append(word_info)

    tf_sum_consp = 0
    tf_sum_nonconsp = 0
    sum_array = []

    for i, word_info in enumerate(total_word_info):
        tf_sum_consp = 0
        tf_sum_nonconsp = 0
        tf_sum_delta = 0

        for array in word_info:
            boolchecker = array[2]

            if boolchecker == 1:

                value = array[1]
                tf_sum_nonconsp += value

            else:
                value = array[1]

                tf_sum_consp += value

        tf_sum_delta = tf_sum_nonconsp - tf_sum_consp

        sum_array.append([feature_names[i], tf_sum_delta])

    deltas = []
    for item in sum_array:
        deltas.append(item[1])

    deltas = np.array(deltas)
    indices = deltas.argsort()[:keywords_number]

    keywords_1 = [sum_array[i] for i in indices]

    keyword_list1 = []
    for i in keywords_1:
        keyword_list1.append(i[0])

    print("there are this many keywords in list1: ", len(keyword_list1))
    # we pickle it for posterity
    os.chdir(filepath_togo)

    with open(str(keywords_num)+'keyword_list1.pickle', 'wb') as f:
        pickle.dump(keyword_list1, f)

    ### now for keyword extraction method 2

    # method two runs a basic pipeline with a SVM then finds most distinguishing features

    os.chdir(filepath_trans)
    with open(filename, 'rb') as f:
        transcripts_cleaned = pickle.load(f)
    print("fully loaded")

    list_document_tokens = []
    for i, document in enumerate(transcripts_cleaned):
        list_document_tokens.append(transcripts_cleaned[i][0])

    tfidf_input = []
    for document in list_document_tokens:
        tfidf_input.append(" ".join(document))

    # now for feature extraction

    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer

    tv = TfidfVectorizer(stop_words=None, max_features=10000)
    word_count_vector = tv.fit_transform(tfidf_input)

    tf_idf_vector = tv.fit_transform(tfidf_input).toarray()

    # create X and y

    X = tf_idf_vector
    y = []

    # merge categories
    for document in transcripts_cleaned:

        class_made = 0
        if document[1] == 1:
            class_made = 0
        else:
            class_made = 1
        y.append(class_made)

    # train test split
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    # Support Vector Machine Classifier
    from sklearn import svm
    classifier3 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
    classifier3.fit(X_train, y_train)

    y_pred3 = classifier3.predict(X_test)
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score


    coef = classifier3.coef_.ravel()
    top_positive_coefficients = np.argsort(coef)[-int((keywords_number)):]
    top_negative_coefficients = np.argsort(coef)[:int(keywords_number)]
    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])
    feature_names = np.array(tv.get_feature_names())
    keyword_list2 = feature_names[top_positive_coefficients]

    print(keyword_list1)
    print(keyword_list2)

    # we pickle it for posterity
    os.chdir(filepath_togo)

    with open(str(keywords_num)+'keyword_list2.pickle', 'wb') as f:
        pickle.dump(keyword_list2, f)

    print("finished extracting keywords")

# # # # KEYWORD EXTRACTION creates two pickle files of keyword top keywords
filepath_trans = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Cleaned data 1'
filepath_togo = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
for k in keyword_number: 
  keywordextractor(filepath_trans, filepath_togo, 'training_data_cleaned.pickle', k)

"""This code is to load all the keyword lists once they are created:"""

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 10
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_10 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_10 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 15
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_15 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_15 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 20
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_20 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_20 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 50
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_50 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_50 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 100
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_100 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_100 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 200
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_200 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_200 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 500
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_500 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_500 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 750
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_750 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_750 = pickle.load(f)

import os
filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Code/CoNLL-2003 code/Keywords 1'
os.chdir(filepath)
import pickle

keyword = 1000
with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:
    keyword_list1_1000 = pickle.load(f)
with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:
    keyword_list2_1000 = pickle.load(f)