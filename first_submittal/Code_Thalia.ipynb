{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_Thalia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe8dU6BD1rvP"
      },
      "source": [
        "import pickle\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkvA4QgC4EXc"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0D9sFaY4JRE"
      },
      "source": [
        "# load training.pkl\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Datasets YouTube/training_data.pickle'\n",
        "with open(path, 'rb') as f:\n",
        "  training = pickle.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-JOD_Gt4plj"
      },
      "source": [
        "# load test.pkl\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Datasets YouTube/test_data.pickle'\n",
        "with open(path, 'rb') as f:\n",
        "  test = pickle.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0K4R_xA4uVR"
      },
      "source": [
        "# load the nlp moduele\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load(disable=[\"parser\", \"tagger\", \"ner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKlLv2pI4uXr"
      },
      "source": [
        "# do lower-case and remove numbers\n",
        "def my_cleaner3(text,nlp):\n",
        "\n",
        "    return[token.lemma_.lower() for token in nlp(text) if not (token.is_alpha==False or len(token.lemma_) <3) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iTJX9Pk4uaa"
      },
      "source": [
        "def cleaner(training_data,nlp,filepath,filename):\n",
        "    '''\n",
        "\n",
        "    :param training_data: input here your list containing training or test dataset\n",
        "    :param nlp: specify variable that holds nlp model\n",
        "    :param filepath: input here your folder where you want the pickle\n",
        "    :param filepath: input here your name for the pickle\n",
        "\n",
        "    :return: returns cleaned version (tokenized and lowercased)\n",
        "    '''\n",
        "\n",
        "    training_data_cleaned = []\n",
        "    for i, transcript in enumerate(training_data):\n",
        "        doc = nlp(training_data[i][0])\n",
        "        #print(\"doc: \", doc)\n",
        "        cleaned_tokens1 = my_cleaner3(doc.text, nlp=nlp)\n",
        "        #print(\"cleaned_tokens1: \", cleaned_tokens1)\n",
        "        training_data_cleaned.append([cleaned_tokens1, training_data[i][1]])\n",
        "        #print(\"training_data_cleaned: \", training_data_cleaned)\n",
        "\n",
        "    import os\n",
        "    os.chdir(filepath)\n",
        "    import pickle\n",
        "    with open(filename+'.pickle', 'wb') as f:\n",
        "        pickle.dump(training_data_cleaned, f)\n",
        "    return training_data_cleaned\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC8OFbZL4z5-"
      },
      "source": [
        "# clean all data and pickle them\n",
        "training_data_cleaned = cleaner(training, nlp,'/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Cleaned data 1', 'training_data_cleaned')\n",
        "test_data_cleaned = cleaner(test, nlp,'/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Cleaned data 1', 'test_data_cleaned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAVRHTUc4ucL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOmevH7GQUcl"
      },
      "source": [
        "After cleaning the raw dataset the data can be loaded with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYRIpF4l4GB0"
      },
      "source": [
        "# open training_data_cleaned & test_data_cleaned without making them first \n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Cleaned data 1'\n",
        "\n",
        "import os\n",
        "os.chdir(filepath)\n",
        "with open('training_data_cleaned.pickle', 'rb') as f: \n",
        "  training_data_cleaned = pickle.load(f)\n",
        "with open('test_data_cleaned.pickle', 'rb') as f: \n",
        "  test_data_cleaned = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuT9Bb-l46jp"
      },
      "source": [
        "# check\n",
        "print(len(training_data_cleaned))\n",
        "print(len(test_data_cleaned))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALpvkPRI49UM"
      },
      "source": [
        "# we determine some hyperparameters here for the rest of the research, here we can change them to see the influence of certain model parameters\n",
        "# as wel as set some key variables\n",
        "keyword_number = [20, 50, 100, 200, 500, 750, 1000]\n",
        "SVM_C = [1, 3, 5, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwlHVdD24_tA"
      },
      "source": [
        "def keywordextractor(filepath_trans, filepath_togo, filename,keywords_num):\n",
        "\n",
        "    \"\"\" this function creates two picklefiles, each has a keyword list\n",
        "    first parameter; filepath = the path where you want the pickles to go, and the path where your transcript data is\n",
        "    filename: name of the pickle: for example; 'cleaned_transcript_data.pickle'\n",
        "    second parameter; keywords_num = amount of keywords you want in your list\"\"\"\n",
        "\n",
        "    import os\n",
        "    import pickle\n",
        "    import numpy as np\n",
        "\n",
        "    os.chdir(filepath_trans)\n",
        "    with open(filename, 'rb') as f:\n",
        "        transcripts_cleaned = pickle.load(f)\n",
        "\n",
        "\n",
        "    keywords_number = keywords_num\n",
        "    ### taking only transcripts\n",
        "    list_document_tokens = []\n",
        "    for i, document in enumerate(transcripts_cleaned):\n",
        "        list_document_tokens.append(transcripts_cleaned[i][0])\n",
        "\n",
        "    # create a list of documents to input to tfidfvectorizer\n",
        "    tfidf_input = []\n",
        "    for document in list_document_tokens:\n",
        "        tfidf_input.append(\" \".join(document))\n",
        "\n",
        "    ### split it by class\n",
        "    list_document_tokens_consp = []\n",
        "    list_document_tokens_nonconsp = []\n",
        "    for i, document in enumerate(transcripts_cleaned):\n",
        "      #print(\"i: \", i)\n",
        "      #print(\"document: \", document)\n",
        "      if document[1] == '1':\n",
        "\n",
        "          list_document_tokens_nonconsp.append(transcripts_cleaned[i][0])\n",
        "      else:\n",
        "          list_document_tokens_consp.append(transcripts_cleaned[i][0])\n",
        "\n",
        "    tfidf_input_consp = []\n",
        "    tfidf_input_nonconsp = []\n",
        "    for document in list_document_tokens_consp:\n",
        "        tfidf_input_consp.append(\" \".join(document))\n",
        "    for document in list_document_tokens_nonconsp:\n",
        "        tfidf_input_nonconsp.append(\" \".join(document))\n",
        "\n",
        "    # now for keyword extraction method 1\n",
        "\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.feature_extraction.text import TfidfTransformer\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "    tv = TfidfVectorizer(stop_words=None, max_features=10000)\n",
        "\n",
        "    vocab = tv.fit(tfidf_input)\n",
        "    feature_names = vocab.get_feature_names()\n",
        "\n",
        "    word_count_vector = tv.fit_transform(tfidf_input).toarray()\n",
        "\n",
        "    word_count_vector_transposed = word_count_vector.T\n",
        "\n",
        "    total_word_info = []\n",
        "    for i, word in enumerate(word_count_vector_transposed):\n",
        "        tempword = word.tolist()\n",
        "        word_info = []\n",
        "\n",
        "        for j, document in enumerate(tempword):\n",
        "            word_info.append([j, document, transcripts_cleaned[j][1]])\n",
        "        total_word_info.append(word_info)\n",
        "\n",
        "    tf_sum_consp = 0\n",
        "    tf_sum_nonconsp = 0\n",
        "    sum_array = []\n",
        "\n",
        "    for i, word_info in enumerate(total_word_info):\n",
        "        tf_sum_consp = 0\n",
        "        tf_sum_nonconsp = 0\n",
        "        tf_sum_delta = 0\n",
        "\n",
        "        for array in word_info:\n",
        "            boolchecker = array[2]\n",
        "\n",
        "            if boolchecker == 1:\n",
        "\n",
        "                value = array[1]\n",
        "                tf_sum_nonconsp += value\n",
        "\n",
        "            else:\n",
        "                value = array[1]\n",
        "\n",
        "                tf_sum_consp += value\n",
        "\n",
        "        tf_sum_delta = tf_sum_nonconsp - tf_sum_consp\n",
        "\n",
        "        sum_array.append([feature_names[i], tf_sum_delta])\n",
        "\n",
        "    deltas = []\n",
        "    for item in sum_array:\n",
        "        deltas.append(item[1])\n",
        "\n",
        "    deltas = np.array(deltas)\n",
        "    indices = deltas.argsort()[:keywords_number]\n",
        "\n",
        "    keywords_1 = [sum_array[i] for i in indices]\n",
        "\n",
        "    keyword_list1 = []\n",
        "    for i in keywords_1:\n",
        "        keyword_list1.append(i[0])\n",
        "\n",
        "    print(\"there are this many keywords in list1: \", len(keyword_list1))\n",
        "    # we pickle it for posterity\n",
        "    os.chdir(filepath_togo)\n",
        "\n",
        "    with open(str(keywords_num)+'keyword_list1.pickle', 'wb') as f:\n",
        "        pickle.dump(keyword_list1, f)\n",
        "\n",
        "    ### now for keyword extraction method 2\n",
        "\n",
        "    # method two runs a basic pipeline with a SVM then finds most distinguishing features\n",
        "\n",
        "    os.chdir(filepath_trans)\n",
        "    with open(filename, 'rb') as f:\n",
        "        transcripts_cleaned = pickle.load(f)\n",
        "    print(\"fully loaded\")\n",
        "\n",
        "    list_document_tokens = []\n",
        "    for i, document in enumerate(transcripts_cleaned):\n",
        "        list_document_tokens.append(transcripts_cleaned[i][0])\n",
        "\n",
        "    tfidf_input = []\n",
        "    for document in list_document_tokens:\n",
        "        tfidf_input.append(\" \".join(document))\n",
        "\n",
        "    # now for feature extraction\n",
        "\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "    tv = TfidfVectorizer(stop_words=None, max_features=10000)\n",
        "    word_count_vector = tv.fit_transform(tfidf_input)\n",
        "\n",
        "    tf_idf_vector = tv.fit_transform(tfidf_input).toarray()\n",
        "\n",
        "    # create X and y\n",
        "\n",
        "    X = tf_idf_vector\n",
        "    y = []\n",
        "\n",
        "    # merge categories\n",
        "    for document in transcripts_cleaned:\n",
        "\n",
        "        class_made = 0\n",
        "        if document[1] == 1:\n",
        "            class_made = 0\n",
        "        else:\n",
        "            class_made = 1\n",
        "        y.append(class_made)\n",
        "\n",
        "    # train test split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Support Vector Machine Classifier\n",
        "    from sklearn import svm\n",
        "    classifier3 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "    classifier3.fit(X_train, y_train)\n",
        "\n",
        "    y_pred3 = classifier3.predict(X_test)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "    coef = classifier3.coef_.ravel()\n",
        "    top_positive_coefficients = np.argsort(coef)[-int((keywords_number)):]\n",
        "    top_negative_coefficients = np.argsort(coef)[:int(keywords_number)]\n",
        "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
        "    feature_names = np.array(tv.get_feature_names())\n",
        "    keyword_list2 = feature_names[top_positive_coefficients]\n",
        "\n",
        "    print(keyword_list1)\n",
        "    print(keyword_list2)\n",
        "\n",
        "    # we pickle it for posterity\n",
        "    os.chdir(filepath_togo)\n",
        "\n",
        "    with open(str(keywords_num)+'keyword_list2.pickle', 'wb') as f:\n",
        "        pickle.dump(keyword_list2, f)\n",
        "\n",
        "    print(\"finished extracting keywords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEzgNH0u5CDk"
      },
      "source": [
        "# # # # KEYWORD EXTRACTION creates two pickle files of keyword top keywords\n",
        "filepath_trans = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Cleaned data 1'\n",
        "filepath_togo = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "for k in keyword_number: \n",
        "  keywordextractor(filepath_trans, filepath_togo, 'training_data_cleaned.pickle', k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4FQiSavQx6z"
      },
      "source": [
        "This code is to load all the keyword lists once they are created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8qk_k0s5C7Y"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 20\n",
        "with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:\n",
        "    keyword_list1_20 = pickle.load(f)\n",
        "with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:\n",
        "    keyword_list2_20 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KReb35wk5H05"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 50\n",
        "with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:\n",
        "    keyword_list1_50 = pickle.load(f)\n",
        "with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:\n",
        "    keyword_list2_50 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j8RbTb25JvD"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 100\n",
        "with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:\n",
        "    keyword_list1_100 = pickle.load(f)\n",
        "with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:\n",
        "    keyword_list2_100 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfL3LpYI5La6"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 200\n",
        "with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:\n",
        "    keyword_list1_200 = pickle.load(f)\n",
        "with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:\n",
        "    keyword_list2_200 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9QaXbff5NXn"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 500\n",
        "with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:\n",
        "    keyword_list1_500 = pickle.load(f)\n",
        "with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:\n",
        "    keyword_list2_500 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob4WatsV5O7Q"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 750\n",
        "with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:\n",
        "    keyword_list1_750 = pickle.load(f)\n",
        "with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:\n",
        "    keyword_list2_750 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sk9UzkM5Qyq"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 1000\n",
        "with open(str(keyword) + 'keyword_list1.pickle', 'rb') as f:\n",
        "    keyword_list1_1000 = pickle.load(f)\n",
        "with open(str(keyword) + 'keyword_list2.pickle', 'rb') as f:\n",
        "    keyword_list2_1000 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qtnWEJNQ3eM"
      },
      "source": [
        "Transform the cleaned dataset into a vectorized list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yqqmwlf5Son"
      },
      "source": [
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6DsBEit5Ut4"
      },
      "source": [
        "# for the train set I create X and y\n",
        "X_trainingdata = []\n",
        "y_trainingdata = []\n",
        "for item in training_data_cleaned:\n",
        "    X_trainingdata.append(item[0])\n",
        "    y_trainingdata.append(item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Z7RyYj5WPg"
      },
      "source": [
        "# Here I transform X to fit the input of the tfidf vectorizer\n",
        "list_document_tokens = []\n",
        "for i, document in enumerate(X_trainingdata):\n",
        "    list_document_tokens.append(training_data_cleaned[i][0])\n",
        "tfidf_input = []\n",
        "for document in list_document_tokens:\n",
        "    tfidf_input.append(\" \".join(document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkknPgeH5X9p"
      },
      "source": [
        "# I do the same for my test data\n",
        "X_testdata = []\n",
        "y_testdata = []\n",
        "for item in test_data_cleaned:\n",
        "    X_testdata.append(item[0])\n",
        "    y_testdata.append(item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNP2sh6T5Zp3"
      },
      "source": [
        "# once again making them fit\n",
        "list_document_tokens_test = []\n",
        "for i, document in enumerate(X_testdata):\n",
        "    list_document_tokens_test.append(test_data_cleaned[i][0])\n",
        "tfidf_input_test = []\n",
        "for document in list_document_tokens_test:\n",
        "    tfidf_input_test.append(\" \".join(document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m3TNAux5bd1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oIZgysk5myR"
      },
      "source": [
        "The next step is to load and train the BERT model. Once the model is pretrained on the NER-labeled dataset, BERT can predict NER tags on the keyword lists. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfaiZYp5xgf"
      },
      "source": [
        "**Clone Repository:**\n",
        "First Step is to clone KAMALKRAJ Github Repository. Below code is command to clone repository:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CM0O5D45oEO"
      },
      "source": [
        "!git clone https://github.com/kamalkraj/BERT-NER.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqaxGhR_5_76"
      },
      "source": [
        "Use \"ls -l\" command for verfying the repository cloned properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJnITgvc5ryM"
      },
      "source": [
        "ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrbsz1596Cn_"
      },
      "source": [
        "Now go to 'BERT-NER' directory by using below command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK6TAKzD6FNB"
      },
      "source": [
        "cd BERT-NER/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mdwz45t6JMQ"
      },
      "source": [
        "**BERT-NER files:** Use 'ls -l' to check content inside BERT-NER folder. These below files and folders we will use for finetuning and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1JLKJNT6L18"
      },
      "source": [
        "ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXZRsStB6NsZ"
      },
      "source": [
        "\"requirements.txt\" contains all the pacakages that required for trainig and inference. By using below command install all the pacakages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ctEDx3n6SCS"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZVlGA2k6UGR"
      },
      "source": [
        "### **Fine-Tuning:**\n",
        "For finetuning or training the **bert-base** model run the 'run_ner.py' as command given below.\n",
        "\n",
        "In below command we have to pass different arguments:\n",
        "   \n",
        "*   '--data_dir' argument required to collect dataset. Pass 'data/' as argument which we can see as directory inside 'BERT-NER' folder for the previous comment and command for 'BERT-NER files' .\n",
        "*   '--bert_model' used to download **pretrained bert base** model of Hugging Face transformers. There are different model-names as suggested by hugging face for argument, here we select 'bert-base-cased'.\n",
        "*   '--task_name' argument used for task to perform. Enter 'ner' as we will train the model for Named Entity Recogintion(NER).\n",
        "*   '--output_dir' argument is for where to store fine-tuned model. We give name 'out_base' for directory  where fine-tuned model stored.\n",
        "*   Other arguments like '--max_seq_length', '--num_train_epochs' and '--warmup_proportion', just give values as suggested in repository.\n",
        "*   For training pass argument '--do_train' and after that evaluating for results pass argument '--do_eval'.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOLhhXrh6W7J"
      },
      "source": [
        "!python run_ner.py --data_dir=data/ --bert_model=bert-base-cased --task_name=ner --output_dir=out_ner --max_seq_length=128 --do_train --num_train_epochs 3 --do_eval --warmup_proportion=0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm99IuwZ6f_B"
      },
      "source": [
        "**Overwrite 'bert.py' files:**\n",
        "In 'bert.py' we have made changes for better representation and display of '**entity detected**' and their '**entity types**' for the given sentence to test or inference.\n",
        ">Overwrite the 'bert.py' by using below command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyKgZKnx6gtm"
      },
      "source": [
        "%%writefile bert.py\n",
        "\"\"\"BERT NER Inference.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from nltk import word_tokenize\n",
        "from pytorch_transformers import (BertConfig, BertForTokenClassification,\n",
        "                                  BertTokenizer)\n",
        "\n",
        "\n",
        "class BertNer(BertForTokenClassification):\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, valid_ids=None):\n",
        "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask, head_mask=None)[0]\n",
        "        batch_size,max_len,feat_dim = sequence_output.shape\n",
        "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        for i in range(batch_size):\n",
        "            jj = -1\n",
        "            for j in range(max_len):\n",
        "                    if valid_ids[i][j].item() == 1:\n",
        "                        jj += 1\n",
        "                        valid_output[i][jj] = sequence_output[i][j]\n",
        "        sequence_output = self.dropout(valid_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        return logits\n",
        "\n",
        "class Ner:\n",
        "\n",
        "    def __init__(self,model_dir: str):\n",
        "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
        "        self.label_map = self.model_config[\"label_map\"]\n",
        "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
        "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
        "        model_config = os.path.join(model_dir,model_config)\n",
        "        model_config = json.load(open(model_config))\n",
        "        model = BertNer.from_pretrained(model_dir)\n",
        "        tokenizer = BertTokenizer.from_pretrained(model_dir, do_lower_case=model_config[\"do_lower\"])\n",
        "        return model, tokenizer, model_config\n",
        "\n",
        "    def tokenize(self, text: str):\n",
        "        \"\"\" tokenize input\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        tokens = []\n",
        "        valid_positions = []\n",
        "        for i,word in enumerate(words):\n",
        "            token = self.tokenizer.tokenize(word)\n",
        "            tokens.extend(token)\n",
        "            for i in range(len(token)):\n",
        "                if i == 0:\n",
        "                    valid_positions.append(1)\n",
        "                else:\n",
        "                    valid_positions.append(0)\n",
        "        return tokens, valid_positions\n",
        "\n",
        "    def preprocess(self, text: str):\n",
        "        \"\"\" preprocess \"\"\"\n",
        "        tokens, valid_positions = self.tokenize(text)\n",
        "        ## insert \"[CLS]\"\n",
        "        tokens.insert(0,\"[CLS]\")\n",
        "        valid_positions.insert(0,1)\n",
        "        ## insert \"[SEP]\"\n",
        "        tokens.append(\"[SEP]\")\n",
        "        valid_positions.append(1)\n",
        "        segment_ids = []\n",
        "        for i in range(len(tokens)):\n",
        "            segment_ids.append(0)\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        while len(input_ids) < self.max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            segment_ids.append(0)\n",
        "            valid_positions.append(0)\n",
        "        return input_ids,input_mask,segment_ids,valid_positions\n",
        "\n",
        "    def predict(self, text: str):\n",
        "        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n",
        "        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
        "        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n",
        "        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n",
        "        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids, segment_ids, input_mask,valid_ids)\n",
        "        logits = F.softmax(logits,dim=2)\n",
        "        logits_label = torch.argmax(logits,dim=2)\n",
        "        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
        "\n",
        "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
        "\n",
        "        logits = []\n",
        "        pos = 0\n",
        "        for index,mask in enumerate(valid_ids[0]):\n",
        "            if index == 0:\n",
        "                continue\n",
        "            if mask == 1:\n",
        "                logits.append((logits_label[index-pos],logits_confidence[index-pos]))\n",
        "            else:\n",
        "                pos += 1\n",
        "        logits.pop()\n",
        "\n",
        "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
        "        words = word_tokenize(text)\n",
        "        assert len(labels) == len(words)\n",
        "\n",
        "        Person = []\n",
        "        Location = []\n",
        "        Organization = []\n",
        "        Miscelleneous = []\n",
        "\n",
        "        for word, (label, confidence) in zip(words, labels):\n",
        "            if label==\"B-PER\" or label==\"I-PER\":\n",
        "                Person.append(word)\n",
        "            elif label==\"B-LOC\" or label==\"I-LOC\":\n",
        "                Location.append(word)\n",
        "            elif label==\"B-ORG\" or label==\"I-ORG\":\n",
        "                Organization.append(word)\n",
        "            elif label==\"B-MISC\" or label==\"I-MISC\":\n",
        "                Miscelleneous.append(word)\n",
        "            else:\n",
        "                output = None\n",
        "\n",
        "        output = []\n",
        "        for word, (label, confidence) in zip(words, labels):      \n",
        "            if label == \"B-PER\":\n",
        "                output.append(' '.join(Person) + \": Person\")\n",
        "            if label==\"B-LOC\":\n",
        "                output.append(' '.join(Location) + \": Location\")\n",
        "            if label==\"B-MISC\":\n",
        "                output.append(' '.join(Miscelleneous) + \": Miscelleneous Entity\")\n",
        "            if label==\"B-ORG\":\n",
        "                output.append(' '.join(Organization) + \": Organization\")\n",
        "                \n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vl2_NNf67QL"
      },
      "source": [
        "Put all the keyword lists into one list in order for BERT to make predictions on it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbVDJMVw66Z6"
      },
      "source": [
        "keyword_lists = [keyword_list1_20, keyword_list2_20, keyword_list1_50, keyword_list2_50, keyword_list1_100,\n",
        "                 keyword_list2_100, keyword_list1_200, keyword_list2_200, keyword_list1_500, keyword_list2_500,\n",
        "                 keyword_list1_750, keyword_list2_750, keyword_list1_1000, keyword_list2_1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CqryYJQ7Luy"
      },
      "source": [
        "Next is some code to replace the first letter of a word with a captical letter, since NER gets a better performance with capital first letters for words referring to a person or organizations. Besides that the keywords are also transformed to input that BERT can take. \n",
        "\n",
        "I will change it all to beautiful for loops for the second submital. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2UkD-0I8mVL"
      },
      "source": [
        "# Python program to convert a list\n",
        "# to string using join() function\n",
        "    \n",
        "# Function to convert  \n",
        "def listToString(s): \n",
        "    \n",
        "    # initialize an empty string\n",
        "    str1 = \" \" \n",
        "    \n",
        "    # return string  \n",
        "    return (str1.join(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmNdiqJM8uKT"
      },
      "source": [
        "k_list1_20 = listToString(keyword_list1_20)\n",
        "k_list2_20 = listToString(keyword_list2_20)\n",
        "k_list1_50 = listToString(keyword_list1_50)\n",
        "k_list2_50 = listToString(keyword_list2_50)\n",
        "k_list1_100 = listToString(keyword_list1_100)\n",
        "k_list2_100 = listToString(keyword_list2_100)\n",
        "k_list1_200 = listToString(keyword_list1_200)\n",
        "k_list2_200 = listToString(keyword_list2_200)\n",
        "k_list1_500 = listToString(keyword_list1_500)\n",
        "k_list2_500 = listToString(keyword_list2_500)\n",
        "k_list1_750 = listToString(keyword_list1_750)\n",
        "k_list2_750 = listToString(keyword_list2_750)\n",
        "k_list1_1000 = listToString(keyword_list1_1000)\n",
        "k_list2_1000 = listToString(keyword_list2_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoH3gJCZ8wRk"
      },
      "source": [
        "k_list1_20 = k_list1_20.title().split()\n",
        "k_list2_20 = k_list2_20.title().split()\n",
        "k_list1_50 = k_list1_50.title().split()\n",
        "k_list2_50 = k_list2_50.title().split()\n",
        "k_list1_100 = k_list1_100.title().split()\n",
        "k_list2_100 = k_list2_100.title().split()\n",
        "k_list1_200 = k_list1_200.title().split()\n",
        "k_list2_200 = k_list2_200.title().split()\n",
        "k_list1_500 = k_list1_500.title().split()\n",
        "k_list2_500 = k_list2_500.title().split()\n",
        "k_list1_750 = k_list1_750.title().split()\n",
        "k_list2_750 = k_list2_750.title().split()\n",
        "k_list1_1000 = k_list1_1000.title().split()\n",
        "k_list2_1000 = k_list2_1000.title().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHdua3lO6pzM"
      },
      "source": [
        "Run the below cell for final output: \n",
        "\n",
        "In the below cell first line we call the '**Ner**' class from the 'bert.py' file. '**Ner**' class intialize our fine-tuned model.\n",
        "\n",
        "Store the model in any variable. In below cell we store our fine-tuned model into 'model' variable.\n",
        "\n",
        "Pass any text as a string for entity detection. \n",
        "\n",
        "Use '**predict**' function of class '**Ner**' for detecting entities for the 'text' and stored it into 'output'. In 'output' variable we have a detected entities of list.\n",
        "\n",
        "Run 'for loop' for list formed 'output'. Print the 'prediction' of 'for loop'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTgCMKt56ttF"
      },
      "source": [
        "from bert import Ner\n",
        "model = Ner(\"out_ner/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4urq9uNs88mo"
      },
      "source": [
        "new_k_list1_20 = []\n",
        "for t in k_list1_20:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list1_20.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gobmFl_V9FYv"
      },
      "source": [
        "new_k_list2_20 = []\n",
        "for t in k_list2_20:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list2_20.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz12Om9q9F1B"
      },
      "source": [
        "new_k_list1_50 = []\n",
        "for t in k_list1_50:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list1_50.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUXt2pXc9F3j"
      },
      "source": [
        "new_k_list2_50 = []\n",
        "for t in k_list2_50:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list2_50.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fRMq8-b9F6B"
      },
      "source": [
        "new_k_list1_100 = []\n",
        "for t in k_list1_100:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list1_100.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2GmAWfI9F85"
      },
      "source": [
        "new_k_list2_100 = []\n",
        "for t in k_list2_100:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list2_100.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa6Jf1eX9F-m"
      },
      "source": [
        "new_k_list1_200 = []\n",
        "for t in k_list1_200:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list1_200.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ric7d2-b9GBM"
      },
      "source": [
        "new_k_list2_200 = []\n",
        "for t in k_list2_200:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list2_200.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfFWOEA49PiL"
      },
      "source": [
        "new_k_list1_500 = []\n",
        "for t in k_list1_500:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list1_500.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1gtc6ap9Pke"
      },
      "source": [
        "new_k_list2_500 = []\n",
        "for t in k_list2_500:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list2_500.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLd1h1Dh9Pmw"
      },
      "source": [
        "new_k_list1_750 = []\n",
        "for t in k_list1_750:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list1_750.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc079RaK9WWT"
      },
      "source": [
        "new_k_list2_750 = []\n",
        "for t in k_list2_750:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list2_750.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSbaGfRc9Wbr"
      },
      "source": [
        "new_k_list1_1000 = []\n",
        "for t in k_list1_1000:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list1_1000.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI-0S7159Pp8"
      },
      "source": [
        "new_k_list2_1000 = []\n",
        "for t in k_list2_1000:\n",
        "  output = model.predict(t)\n",
        "  for p in output:\n",
        "    new_k_list2_1000.append(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj3krJkwSRqR"
      },
      "source": [
        "The NER-tags are now extracted on the keyword lists. Only the person and organization tags will be used in the current research. The following code makes sure the NER-keyword lists have the same format again as the regular keyword lists and the person-and organization NER tags are merged into one list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTGmdXh9gZa"
      },
      "source": [
        "person_keywords_1_20 = []\n",
        "organization_keywords_1_20 = []\n",
        "\n",
        "for match in new_k_list1_20:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_1_20.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_1_20.append(match)\n",
        "\n",
        "print(person_keywords_1_20)\n",
        "print(organization_keywords_1_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iepgmpb09ieU"
      },
      "source": [
        "person_keywords_2_20 = []\n",
        "organization_keywords_2_20 = []\n",
        "\n",
        "for match in new_k_list2_20:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_2_20.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_2_20.append(match)\n",
        "\n",
        "print(person_keywords_2_20)\n",
        "print(organization_keywords_2_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyrpHxgK9qvL"
      },
      "source": [
        "person_keywords_1_50 = []\n",
        "organization_keywords_1_50 = []\n",
        "\n",
        "for match in new_k_list1_50:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_1_50.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_1_50.append(match)\n",
        "\n",
        "print(person_keywords_1_50)\n",
        "print(organization_keywords_1_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RshOCAWL9sPA"
      },
      "source": [
        "person_keywords_2_50 = []\n",
        "organization_keywords_2_50 = []\n",
        "\n",
        "for match in new_k_list2_50:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_2_50.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_2_50.append(match)\n",
        "\n",
        "print(person_keywords_2_50)\n",
        "print(organization_keywords_2_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WpBLVio9uky"
      },
      "source": [
        "person_keywords_1_100 = []\n",
        "organization_keywords_1_100 = []\n",
        "\n",
        "for match in new_k_list1_100:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_1_100.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_1_100.append(match)\n",
        "\n",
        "print(person_keywords_1_100)\n",
        "print(organization_keywords_1_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEsedpik9unW"
      },
      "source": [
        "person_keywords_2_100 = []\n",
        "organization_keywords_2_100 = []\n",
        "\n",
        "for match in new_k_list2_100:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_2_100.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_2_100.append(match)\n",
        "\n",
        "print(person_keywords_2_100)\n",
        "print(organization_keywords_2_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KA9-XED9up9"
      },
      "source": [
        "person_keywords_1_200 = []\n",
        "organization_keywords_1_200 = []\n",
        "\n",
        "for match in new_k_list1_200:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_1_200.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_1_200.append(match)\n",
        "\n",
        "print(person_keywords_1_200)\n",
        "print(organization_keywords_1_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XLAQAr79us2"
      },
      "source": [
        "person_keywords_2_200 = []\n",
        "organization_keywords_2_200 = []\n",
        "\n",
        "for match in new_k_list2_200:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_2_200.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_2_200.append(match)\n",
        "\n",
        "print(person_keywords_2_200)\n",
        "print(organization_keywords_2_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVEBcIzy9uvK"
      },
      "source": [
        "person_keywords_1_500 = []\n",
        "organization_keywords_1_500 = []\n",
        "\n",
        "for match in new_k_list1_500:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_1_500.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_1_500.append(match)\n",
        "\n",
        "print(person_keywords_1_500)\n",
        "print(organization_keywords_1_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD6NabCd9uxo"
      },
      "source": [
        "person_keywords_2_500 = []\n",
        "organization_keywords_2_500 = []\n",
        "\n",
        "for match in new_k_list2_500:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_2_500.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_2_500.append(match)\n",
        "\n",
        "print(person_keywords_2_500)\n",
        "print(organization_keywords_2_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm1ru5Oa94p5"
      },
      "source": [
        "person_keywords_1_750 = []\n",
        "organization_keywords_1_750 = []\n",
        "\n",
        "for match in new_k_list1_750:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_1_750.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_1_750.append(match)\n",
        "\n",
        "print(person_keywords_1_750)\n",
        "print(organization_keywords_1_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZLAJM0d94sW"
      },
      "source": [
        "person_keywords_2_750 = []\n",
        "organization_keywords_2_750 = []\n",
        "\n",
        "for match in new_k_list2_750:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_2_750.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_2_750.append(match)\n",
        "\n",
        "print(person_keywords_2_750)\n",
        "print(organization_keywords_2_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW4Xn0uX94u0"
      },
      "source": [
        "person_keywords_1_1000 = []\n",
        "organization_keywords_1_1000 = []\n",
        "\n",
        "for match in new_k_list1_1000:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_1_1000.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_1_1000.append(match)\n",
        "\n",
        "print(person_keywords_1_1000)\n",
        "print(organization_keywords_1_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSFYTwg89uze"
      },
      "source": [
        "person_keywords_2_1000 = []\n",
        "organization_keywords_2_1000 = []\n",
        "\n",
        "for match in new_k_list2_1000:\n",
        "    if \"Person\" in match:\n",
        "      person_keywords_2_1000.append(match)\n",
        "    if \"Organization\" in match:\n",
        "      organization_keywords_2_1000.append(match)\n",
        "\n",
        "print(person_keywords_2_1000)\n",
        "print(organization_keywords_2_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDue9UIZ9--Y"
      },
      "source": [
        "person_1_20 = []\n",
        "for p in person_keywords_1_20:\n",
        "  p = str(p)\n",
        "  person_1_20.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_1_20 = []\n",
        "for o in organization_keywords_1_20:\n",
        "  o = str(o)\n",
        "  organization_1_20.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLUSl-Vv-GGU"
      },
      "source": [
        "person_2_20 = []\n",
        "for p in person_keywords_2_20:\n",
        "  p = str(p)\n",
        "  person_2_20.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_2_20 = []\n",
        "for o in organization_keywords_2_20:\n",
        "  o = str(o)\n",
        "  organization_2_20.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF0XDgc5-HoE"
      },
      "source": [
        "person_1_50 = []\n",
        "for p in person_keywords_1_50:\n",
        "  p = str(p)\n",
        "  person_1_50.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_1_50 = []\n",
        "for o in organization_keywords_1_50:\n",
        "  o = str(o)\n",
        "  organization_1_50.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs6KSkCP-JVG"
      },
      "source": [
        "person_2_50 = []\n",
        "for p in person_keywords_2_50:\n",
        "  p = str(p)\n",
        "  person_2_50.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_2_50 = []\n",
        "for o in organization_keywords_2_50:\n",
        "  o = str(o)\n",
        "  organization_2_50.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cySgdBah-JXo"
      },
      "source": [
        "person_1_100 = []\n",
        "for p in person_keywords_1_100:\n",
        "  p = str(p)\n",
        "  person_1_100.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_1_100 = []\n",
        "for o in organization_keywords_1_100:\n",
        "  o = str(o)\n",
        "  organization_1_100.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rok6SCG-JZr"
      },
      "source": [
        "person_2_100 = []\n",
        "for p in person_keywords_2_100:\n",
        "  p = str(p)\n",
        "  person_2_100.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_2_100 = []\n",
        "for o in organization_keywords_2_100:\n",
        "  o = str(o)\n",
        "  organization_2_100.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1WhiKHN-Jb7"
      },
      "source": [
        "person_1_200 = []\n",
        "for p in person_keywords_1_200:\n",
        "  p = str(p)\n",
        "  person_1_200.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_1_200 = []\n",
        "for o in organization_keywords_1_200:\n",
        "  o = str(o)\n",
        "  organization_1_200.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yxbu0vX-Jfr"
      },
      "source": [
        "person_2_200 = []\n",
        "for p in person_keywords_2_200:\n",
        "  p = str(p)\n",
        "  person_2_200.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_2_200 = []\n",
        "for o in organization_keywords_2_200:\n",
        "  o = str(o)\n",
        "  organization_2_200.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlMATWsq-T9T"
      },
      "source": [
        "person_1_500 = []\n",
        "for p in person_keywords_1_500:\n",
        "  p = str(p)\n",
        "  person_1_500.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_1_500 = []\n",
        "for o in organization_keywords_1_500:\n",
        "  o = str(o)\n",
        "  organization_1_500.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDi0sO__-T_y"
      },
      "source": [
        "person_2_500 = []\n",
        "for p in person_keywords_2_500:\n",
        "  p = str(p)\n",
        "  person_2_500.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_2_500 = []\n",
        "for o in organization_keywords_2_500:\n",
        "  o = str(o)\n",
        "  organization_2_500.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU5fDfbz-UHd"
      },
      "source": [
        "person_1_750 = []\n",
        "for p in person_keywords_1_750:\n",
        "  p = str(p)\n",
        "  person_1_750.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_1_750 = []\n",
        "for o in organization_keywords_1_750:\n",
        "  o = str(o)\n",
        "  organization_1_750.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daq2kTWG-UOY"
      },
      "source": [
        "person_2_750 = []\n",
        "for p in person_keywords_2_750:\n",
        "  p = str(p)\n",
        "  person_2_750.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_2_750 = []\n",
        "for o in organization_keywords_2_750:\n",
        "  o = str(o)\n",
        "  organization_2_750.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_VYJz1v-a8l"
      },
      "source": [
        "person_1_1000 = []\n",
        "for p in person_keywords_1_1000:\n",
        "  p = str(p)\n",
        "  person_1_1000.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_1_1000 = []\n",
        "for o in organization_keywords_1_1000:\n",
        "  o = str(o)\n",
        "  organization_1_1000.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUzCC4hx-a_b"
      },
      "source": [
        "person_2_1000 = []\n",
        "for p in person_keywords_2_1000:\n",
        "  p = str(p)\n",
        "  person_2_1000.append(p[:p.index(\":\")])\n",
        "\n",
        "organization_2_1000 = []\n",
        "for o in organization_keywords_2_1000:\n",
        "  o = str(o)\n",
        "  organization_2_1000.append(o[:o.index(\":\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehgj9PhkStkb"
      },
      "source": [
        "The following code is to save the NER-keyword lists. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUdAs9D4-goo"
      },
      "source": [
        "ner_key_list1_20 = person_1_20 + organization_1_20\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list1_20.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list1_20, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqFTP_hq-hNc"
      },
      "source": [
        "ner_key_list2_20 = person_2_20 + organization_2_20\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list2_20.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list2_20, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIX_weY-hQh"
      },
      "source": [
        "ner_key_list1_50 = person_1_50 + organization_1_50\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list1_50.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list1_50, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfo8fhtp-hW3"
      },
      "source": [
        "ner_key_list2_50 = person_2_50 + organization_2_50\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list2_50.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list2_50, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvCmKlWb-hY6"
      },
      "source": [
        "ner_key_list1_100 = person_1_100 + organization_1_100\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list1_100.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list1_100, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWGTE6t7-haw"
      },
      "source": [
        "ner_key_list2_100 = person_2_100 + organization_2_100\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list2_100.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list2_100, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF7ghFGl-qOd"
      },
      "source": [
        "ner_key_list1_200 = person_1_200 + organization_1_200\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list1_200.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list1_200, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyQundDc-qRO"
      },
      "source": [
        "ner_key_list2_200 = person_2_200 + organization_2_200\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list2_200.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list2_200, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjatWh1u-qT8"
      },
      "source": [
        "ner_key_list1_500 = person_1_500 + organization_1_500\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list1_500.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list1_500, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKYKV44d-qWO"
      },
      "source": [
        "ner_key_list2_500 = person_2_500 + organization_2_500\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list2_500.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list2_500, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06DIJtsG-qYj"
      },
      "source": [
        "ner_key_list1_750 = person_1_750 + organization_1_750\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list1_750.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list1_750, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Uiv-Jp-qav"
      },
      "source": [
        "ner_key_list2_750 = person_2_750 + organization_2_750\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list2_750.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list2_750, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RHCx68h-0QD"
      },
      "source": [
        "ner_key_list1_1000 = person_1_1000 + organization_1_1000\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list1_1000.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list1_1000, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyfQpA0t-0Sr"
      },
      "source": [
        "ner_key_list2_1000 = person_2_1000 + organization_2_1000\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003\")\n",
        "with open('ner_key_list2_1000.pickle', 'wb') as f:\n",
        "  pickle.dump(ner_key_list2_1000, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrIRCSrF-0U-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pyFpyWW_YrX"
      },
      "source": [
        "Once the NER keyword lists are created and saved, the following code can be used to load it. The first letter of each word is returned to a lower letter again, to ensure the keyword lists and NER-keyword lists have the same format.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy6EkqRs-0XX"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 20\n",
        "with open('ner_key_list1_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list1_20 = pickle.load(f)\n",
        "with open('ner_key_list2_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list2_20 = pickle.load(f)\n",
        "\n",
        "ner_keyword_list1_20 = [x.lower() for x in ner_keyword_list1_20]\n",
        "ner_keyword_list2_20 = [x.lower() for x in ner_keyword_list2_20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFK1OO_V-0aA"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 50\n",
        "with open('ner_key_list1_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list1_50 = pickle.load(f)\n",
        "with open('ner_key_list2_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list2_50 = pickle.load(f)\n",
        "  \n",
        "ner_keyword_list1_50 = [x.lower() for x in ner_keyword_list1_50]\n",
        "ner_keyword_list2_50 = [x.lower() for x in ner_keyword_list2_50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HnyrnWe_yNu"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 100\n",
        "with open('ner_key_list1_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list1_100 = pickle.load(f)\n",
        "with open('ner_key_list2_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list2_100 = pickle.load(f)\n",
        "\n",
        "ner_keyword_list1_100 = [x.lower() for x in ner_keyword_list1_100]\n",
        "ner_keyword_list2_100 = [x.lower() for x in ner_keyword_list2_100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2ZrPPSv_2V-"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 200\n",
        "with open('ner_key_list1_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list1_200 = pickle.load(f)\n",
        "with open('ner_key_list2_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list2_200 = pickle.load(f)\n",
        "\n",
        "ner_keyword_list1_200 = [x.lower() for x in ner_keyword_list1_200]\n",
        "ner_keyword_list2_200 = [x.lower() for x in ner_keyword_list2_200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI1E0Cew_68G"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 500\n",
        "with open('ner_key_list1_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list1_500 = pickle.load(f)\n",
        "with open('ner_key_list2_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list2_500 = pickle.load(f)\n",
        "\n",
        "ner_keyword_list1_500 = [x.lower() for x in ner_keyword_list1_500]\n",
        "ner_keyword_list2_500 = [x.lower() for x in ner_keyword_list2_500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUkbfEIa_-yr"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 750\n",
        "with open('ner_key_list1_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list1_750 = pickle.load(f)\n",
        "with open('ner_key_list2_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list2_750 = pickle.load(f)\n",
        "\n",
        "ner_keyword_list1_750 = [x.lower() for x in ner_keyword_list1_750]\n",
        "ner_keyword_list2_750 = [x.lower() for x in ner_keyword_list2_750]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeEPIzwnADR5"
      },
      "source": [
        "import os\n",
        "filepath = '/content/drive/My Drive/Colab Notebooks/Thesis Project/Data exploration/Keywords 1 NER CONLL2003'\n",
        "os.chdir(filepath)\n",
        "import pickle\n",
        "\n",
        "keyword = 1000\n",
        "with open('ner_key_list1_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list1_1000 = pickle.load(f)\n",
        "with open('ner_key_list2_' + str(keyword) + '.pickle', 'rb') as f:\n",
        "    ner_keyword_list2_1000 = pickle.load(f)\n",
        "\n",
        "ner_keyword_list1_1000 = [x.lower() for x in ner_keyword_list1_1000]\n",
        "ner_keyword_list2_1000 = [x.lower() for x in ner_keyword_list2_1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL04fkySAGl9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpzLtq6PTRPs"
      },
      "source": [
        "Transformation of the training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_0QA9lAAMBN"
      },
      "source": [
        "#for training I fit and transform the train data:\n",
        "tv_training = TfidfVectorizer(stop_words=None, max_features=4000)\n",
        "tf_idf_prel_training = tv_training.fit_transform(tfidf_input)\n",
        "tf_idf_vector_training = tf_idf_prel_training.toarray()\n",
        "tf_idf_feature_names_training = tv_training.get_feature_names()\n",
        "\n",
        "#for test, using the same vectorizer I only transform the test data\n",
        "tf_idf_prel_test = tv_training.transform(tfidf_input_test)\n",
        "tf_idf_vector_test = tf_idf_prel_test.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9xIN0ScAMvl"
      },
      "source": [
        "### We route this back in X and Y variables (X_val & y_val for the validation stage, X_ts0 & y_tst for the final test set)\n",
        "#\n",
        "X_val = tf_idf_vector_training\n",
        "y_val = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBmZDBcSARwe"
      },
      "source": [
        "# Here we condense class 2 & 3 into one class, leaving is with 0 (non conspiratorial) or 1 (conspiratorial)\n",
        "for document in y_trainingdata:\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val.append(class_made)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEEZPDB_ATom"
      },
      "source": [
        "import numpy as np\n",
        "y_val = np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BegJQ9K_AYKA"
      },
      "source": [
        "X_tst = tf_idf_vector_test\n",
        "y_tst = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst.append(class_made)\n",
        "\n",
        "y_tst = np.array(y_tst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHYxCMzYApVH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc02iktUAtj-"
      },
      "source": [
        "**BIAS 1 - BASE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5gQaM2wCC_t"
      },
      "source": [
        "# we need gensims word2vec here\n",
        "import gensim\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvBPCqgUB69C"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2ggsJXJAxC0"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1_20 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_20:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1_20.append(counter)\n",
        "\n",
        "# I now have a matrix containing a multiplier score for each word in the TF-IDF Vocabulary, based on similarity to our keyword lists\n",
        "# now I multiply that matrix with each row of our TF-IDF vectors in both training and test sets\n",
        "X_val_biased_1_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias1_20[j]*item)\n",
        "\n",
        "    X_val_biased_1_20.append(temp_num_20)\n",
        "X_val_biased_1_20 = np.array(X_val_biased_1_20)\n",
        "\n",
        "X_test_biased_1_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias1_20[j]*item)\n",
        "\n",
        "    X_test_biased_1_20.append(temp_num_20)\n",
        "X_test_biased_1_20 = np.array(X_test_biased_1_20)\n",
        "\n",
        "# Now I route this back in X and Y variables\n",
        "#for training:\n",
        "X_val_bias1_20 = X_val_biased_1_20\n",
        "y_val_bias1_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1_20.append(class_made)\n",
        "\n",
        "y_val_bias1_20 = np.array(y_val_bias1_20)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1_20 = X_test_biased_1_20\n",
        "y_tst_bias1_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1_20.append(class_made)\n",
        "\n",
        "y_tst_bias1_20 = np.array(y_tst_bias1_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA2L_L9ACXab"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4nCIuAECV4M"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1_50 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_50:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1_50.append(counter)\n",
        "\n",
        "X_val_biased_1_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias1_50[j]*item)\n",
        "\n",
        "    X_val_biased_1_50.append(temp_num_50)\n",
        "X_val_biased_1_50 = np.array(X_val_biased_1_50)\n",
        "\n",
        "X_test_biased_1_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias1_50[j]*item)\n",
        "\n",
        "    X_test_biased_1_50.append(temp_num_50)\n",
        "X_test_biased_1_50 = np.array(X_test_biased_1_50)\n",
        "\n",
        "X_val_bias1_50 = X_val_biased_1_50\n",
        "y_val_bias1_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1_50.append(class_made)\n",
        "\n",
        "y_val_bias1_50 = np.array(y_val_bias1_50)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1_50 = X_test_biased_1_50\n",
        "y_tst_bias1_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1_50.append(class_made)\n",
        "\n",
        "y_tst_bias1_50 = np.array(y_tst_bias1_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8JfER0yClfG"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txXxAqwmCoIl"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1_100 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_100:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1_100.append(counter)\n",
        "\n",
        "X_val_biased_1_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias1_100[j]*item)\n",
        "\n",
        "    X_val_biased_1_100.append(temp_num_100)\n",
        "X_val_biased_1_100 = np.array(X_val_biased_1_100)\n",
        "\n",
        "X_test_biased_1_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias1_100[j]*item)\n",
        "\n",
        "    X_test_biased_1_100.append(temp_num_100)\n",
        "X_test_biased_1_100 = np.array(X_test_biased_1_100)\n",
        "\n",
        "X_val_bias1_100 = X_val_biased_1_100\n",
        "y_val_bias1_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1_100.append(class_made)\n",
        "\n",
        "y_val_bias1_100 = np.array(y_val_bias1_100)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1_100 = X_test_biased_1_100\n",
        "y_tst_bias1_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1_100.append(class_made)\n",
        "\n",
        "y_tst_bias1_100 = np.array(y_tst_bias1_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oilNiBVQC01W"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C1vNupNC39m"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1_200 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_200:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1_200.append(counter)\n",
        "\n",
        "X_val_biased_1_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias1_200[j]*item)\n",
        "\n",
        "    X_val_biased_1_200.append(temp_num_200)\n",
        "X_val_biased_1_200 = np.array(X_val_biased_1_200)\n",
        "\n",
        "X_test_biased_1_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias1_200[j]*item)\n",
        "\n",
        "    X_test_biased_1_200.append(temp_num_200)\n",
        "X_test_biased_1_200 = np.array(X_test_biased_1_200)\n",
        "\n",
        "X_val_bias1_200 = X_val_biased_1_200\n",
        "y_val_bias1_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1_200.append(class_made)\n",
        "\n",
        "y_val_bias1_200 = np.array(y_val_bias1_200)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1_200 = X_test_biased_1_200\n",
        "y_tst_bias1_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1_200.append(class_made)\n",
        "\n",
        "y_tst_bias1_200 = np.array(y_tst_bias1_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nriJ3q_uDD0n"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBq6aEY4DFzb"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1_500 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_500:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1_500.append(counter)\n",
        "\n",
        "X_val_biased_1_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias1_500[j]*item)\n",
        "\n",
        "    X_val_biased_1_500.append(temp_num_500)\n",
        "X_val_biased_1_500 = np.array(X_val_biased_1_500)\n",
        "\n",
        "X_test_biased_1_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias1_500[j]*item)\n",
        "\n",
        "    X_test_biased_1_500.append(temp_num_500)\n",
        "X_test_biased_1_500 = np.array(X_test_biased_1_500)\n",
        "\n",
        "X_val_bias1_500 = X_val_biased_1_500\n",
        "y_val_bias1_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1_500.append(class_made)\n",
        "\n",
        "y_val_bias1_500 = np.array(y_val_bias1_500)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1_500 = X_test_biased_1_500\n",
        "y_tst_bias1_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1_500.append(class_made)\n",
        "\n",
        "y_tst_bias1_500 = np.array(y_tst_bias1_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umcHW_q0EUIt"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukQ4tV3IEV3U"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1_750 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_750:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1_750.append(counter)\n",
        "  \n",
        "X_val_biased_1_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias1_750[j]*item)\n",
        "\n",
        "    X_val_biased_1_750.append(temp_num_750)\n",
        "X_val_biased_1_750 = np.array(X_val_biased_1_750)\n",
        "\n",
        "X_test_biased_1_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias1_750[j]*item)\n",
        "\n",
        "    X_test_biased_1_750.append(temp_num_750)\n",
        "X_test_biased_1_750 = np.array(X_test_biased_1_750)\n",
        "\n",
        "X_val_bias1_750 = X_val_biased_1_750\n",
        "y_val_bias1_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1_750.append(class_made)\n",
        "\n",
        "y_val_bias1_750 = np.array(y_val_bias1_750)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1_750 = X_test_biased_1_750\n",
        "y_tst_bias1_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1_750.append(class_made)\n",
        "\n",
        "y_tst_bias1_750 = np.array(y_tst_bias1_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Cm-SP5EiEW"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JDcQiO0EjjR"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1_1000 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_1000:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1_1000.append(counter)\n",
        "\n",
        "X_val_biased_1_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias1_1000[j]*item)\n",
        "\n",
        "    X_val_biased_1_1000.append(temp_num_1000)\n",
        "X_val_biased_1_1000 = np.array(X_val_biased_1_1000)\n",
        "\n",
        "X_test_biased_1_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias1_1000[j]*item)\n",
        "\n",
        "    X_test_biased_1_1000.append(temp_num_1000)\n",
        "X_test_biased_1_1000 = np.array(X_test_biased_1_1000)\n",
        "\n",
        "X_val_bias1_1000 = X_val_biased_1_1000\n",
        "y_val_bias1_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1_1000.append(class_made)\n",
        "\n",
        "y_val_bias1_1000 = np.array(y_val_bias1_1000)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1_1000 = X_test_biased_1_1000\n",
        "y_tst_bias1_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1_1000.append(class_made)\n",
        "\n",
        "y_tst_bias1_1000 = np.array(y_tst_bias1_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh-zAOaSEvXY"
      },
      "source": [
        "## **SVM BIAS 1 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPibTcL9Eyl5"
      },
      "source": [
        "X_validation = X_val_bias1_20, X_val_bias1_50, X_val_bias1_100, X_val_bias1_200, X_val_bias1_500, X_val_bias1_750, X_val_bias1_1000\n",
        "y_validation = y_val_bias1_20, y_val_bias1_50, y_val_bias1_100, y_val_bias1_200, y_val_bias1_500, y_val_bias1_750, y_val_bias1_1000\n",
        "\n",
        "X_tst_bias1 = X_tst_bias1_20, X_tst_bias1_50, X_tst_bias1_100, X_tst_bias1_200, X_tst_bias1_500, X_tst_bias1_750, X_tst_bias1_1000\n",
        "y_tst_bias1 = y_tst_bias1_20, y_tst_bias1_50, y_tst_bias1_100, y_tst_bias1_200, y_tst_bias1_500, y_tst_bias1_750, y_tst_bias1_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeP0quzfE0dN"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validation, y_validation, X_tst_bias1, y_tst_bias1):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier2 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier2.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias1\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred2 = classifier2.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred2))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred2))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred2, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred2, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred2))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias1\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred2_test = classifier2.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred2_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred2_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred2_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred2_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred2_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred2_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EhZ04YGFRS3"
      },
      "source": [
        "**BIAS 2 - BASE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7-rMPI1FiE5"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkjVPYrUFTqH"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2_20 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_20:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2_20.append(counter)\n",
        "\n",
        "X_val_biased_2_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias2_20[j]*item)\n",
        "\n",
        "    X_val_biased_2_20.append(temp_num_20)\n",
        "X_val_biased_2_20 = np.array(X_val_biased_2_20)\n",
        "\n",
        "X_test_biased_2_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias2_20[j]*item)\n",
        "\n",
        "    X_test_biased_2_20.append(temp_num_20)\n",
        "X_test_biased_2_20 = np.array(X_test_biased_2_20)\n",
        "\n",
        "X_val_bias2_20 = X_val_biased_2_20\n",
        "y_val_bias2_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2_20.append(class_made)\n",
        "\n",
        "y_val_bias2_20 = np.array(y_val_bias2_20)\n",
        "\n",
        "X_tst_bias2_20 = X_test_biased_2_20\n",
        "y_tst_bias2_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2_20.append(class_made)\n",
        "\n",
        "y_tst_bias2_20 = np.array(y_tst_bias2_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OKLQPWBFgcI"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x13ltB1FmCZ"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2_50 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_50:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2_50.append(counter)\n",
        "\n",
        "X_val_biased_2_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias2_50[j]*item)\n",
        "\n",
        "    X_val_biased_2_50.append(temp_num_50)\n",
        "X_val_biased_2_50 = np.array(X_val_biased_2_50)\n",
        "\n",
        "X_test_biased_2_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias2_50[j]*item)\n",
        "\n",
        "    X_test_biased_2_50.append(temp_num_50)\n",
        "X_test_biased_2_50 = np.array(X_test_biased_2_50)\n",
        "\n",
        "X_val_bias2_50 = X_val_biased_2_50\n",
        "y_val_bias2_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2_50.append(class_made)\n",
        "\n",
        "y_val_bias2_50 = np.array(y_val_bias2_50)\n",
        "\n",
        "X_tst_bias2_50 = X_test_biased_2_50\n",
        "y_tst_bias2_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2_50.append(class_made)\n",
        "\n",
        "y_tst_bias2_50 = np.array(y_tst_bias2_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ybSgYMQFxqE"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdAe8Fu3FzIP"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2_100 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_100:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2_100.append(counter)\n",
        "\n",
        "X_val_biased_2_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias2_100[j]*item)\n",
        "\n",
        "    X_val_biased_2_100.append(temp_num_100)\n",
        "X_val_biased_2_100 = np.array(X_val_biased_2_100)\n",
        "\n",
        "X_test_biased_2_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias2_100[j]*item)\n",
        "\n",
        "    X_test_biased_2_100.append(temp_num_100)\n",
        "X_test_biased_2_100 = np.array(X_test_biased_2_100)\n",
        "\n",
        "X_val_bias2_100 = X_val_biased_2_100\n",
        "y_val_bias2_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2_100.append(class_made)\n",
        "\n",
        "y_val_bias2_100 = np.array(y_val_bias2_100)\n",
        "\n",
        "X_tst_bias2_100 = X_test_biased_2_100\n",
        "y_tst_bias2_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2_100.append(class_made)\n",
        "\n",
        "y_tst_bias2_100 = np.array(y_tst_bias2_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmBKvh0wF-YF"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgM3nxp_F_v2"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2_200 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_200:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2_200.append(counter)\n",
        "\n",
        "X_val_biased_2_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias2_200[j]*item)\n",
        "\n",
        "    X_val_biased_2_200.append(temp_num_200)\n",
        "X_val_biased_2_200 = np.array(X_val_biased_2_200)\n",
        "\n",
        "X_test_biased_2_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias2_200[j]*item)\n",
        "\n",
        "    X_test_biased_2_200.append(temp_num_200)\n",
        "X_test_biased_2_200 = np.array(X_test_biased_2_200)\n",
        "\n",
        "X_val_bias2_200 = X_val_biased_2_200\n",
        "y_val_bias2_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2_200.append(class_made)\n",
        "\n",
        "y_val_bias2_200 = np.array(y_val_bias2_200)\n",
        "\n",
        "X_tst_bias2_200 = X_test_biased_2_200\n",
        "y_tst_bias2_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2_200.append(class_made)\n",
        "\n",
        "y_tst_bias2_200 = np.array(y_tst_bias2_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt-Dai5xGLR6"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijT8Mz30GNLk"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2_500 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_500:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2_500.append(counter)\n",
        "\n",
        "X_val_biased_2_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias2_500[j]*item)\n",
        "\n",
        "    X_val_biased_2_500.append(temp_num_500)\n",
        "X_val_biased_2_500 = np.array(X_val_biased_2_500)\n",
        "\n",
        "X_test_biased_2_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias2_500[j]*item)\n",
        "\n",
        "    X_test_biased_2_500.append(temp_num_500)\n",
        "X_test_biased_2_500 = np.array(X_test_biased_2_500)\n",
        "\n",
        "X_val_bias2_500 = X_val_biased_2_500\n",
        "y_val_bias2_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2_500.append(class_made)\n",
        "\n",
        "y_val_bias2_500 = np.array(y_val_bias2_500)\n",
        "\n",
        "X_tst_bias2_500 = X_test_biased_2_500\n",
        "y_tst_bias2_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2_500.append(class_made)\n",
        "\n",
        "y_tst_bias2_500 = np.array(y_tst_bias2_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImNKePU8GXMB"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAvm9ZqBGY0T"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2_750 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_750:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2_750.append(counter)\n",
        "\n",
        "X_val_biased_2_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias2_750[j]*item)\n",
        "\n",
        "    X_val_biased_2_750.append(temp_num_750)\n",
        "X_val_biased_2_750 = np.array(X_val_biased_2_750)\n",
        "\n",
        "X_test_biased_2_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias2_750[j]*item)\n",
        "\n",
        "    X_test_biased_2_750.append(temp_num_750)\n",
        "X_test_biased_2_750 = np.array(X_test_biased_2_750)\n",
        "\n",
        "X_val_bias2_750 = X_val_biased_2_750\n",
        "y_val_bias2_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2_750.append(class_made)\n",
        "\n",
        "y_val_bias2_750 = np.array(y_val_bias2_750)\n",
        "\n",
        "X_tst_bias2_750 = X_test_biased_2_750\n",
        "y_tst_bias2_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2_750.append(class_made)\n",
        "\n",
        "y_tst_bias2_750 = np.array(y_tst_bias2_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNcV4wq4Gja1"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQyOlDstGlV1"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2_1000 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_1000:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2_1000.append(counter)\n",
        "\n",
        "X_val_biased_2_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias2_1000[j]*item)\n",
        "\n",
        "    X_val_biased_2_1000.append(temp_num_1000)\n",
        "X_val_biased_2_1000 = np.array(X_val_biased_2_1000)\n",
        "\n",
        "X_test_biased_2_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias2_1000[j]*item)\n",
        "\n",
        "    X_test_biased_2_1000.append(temp_num_1000)\n",
        "X_test_biased_2_1000 = np.array(X_test_biased_2_1000)\n",
        "\n",
        "X_val_bias2_1000 = X_val_biased_2_1000\n",
        "y_val_bias2_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2_1000.append(class_made)\n",
        "\n",
        "y_val_bias2_1000 = np.array(y_val_bias2_1000)\n",
        "\n",
        "X_tst_bias2_1000 = X_test_biased_2_1000\n",
        "y_tst_bias2_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2_1000.append(class_made)\n",
        "\n",
        "y_tst_bias2_1000 = np.array(y_tst_bias2_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4vgAAaPGyVw"
      },
      "source": [
        "## **SVM BIAS 2 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H112t2LcG2E1"
      },
      "source": [
        "X_validation = X_val_bias2_20, X_val_bias2_50, X_val_bias2_100, X_val_bias2_200, X_val_bias2_500, X_val_bias2_750, X_val_bias2_1000\n",
        "y_validation = y_val_bias2_20, y_val_bias2_50, y_val_bias2_100, y_val_bias2_200, y_val_bias2_500, y_val_bias2_750, y_val_bias2_1000\n",
        "\n",
        "X_tst_bias2 = X_tst_bias2_20, X_tst_bias2_50, X_tst_bias2_100, X_tst_bias2_200, X_tst_bias2_500, X_tst_bias2_750, X_tst_bias2_1000\n",
        "y_tst_bias2 = y_tst_bias2_20, y_tst_bias2_50, y_tst_bias2_100, y_tst_bias2_200, y_tst_bias2_500, y_tst_bias2_750, y_tst_bias2_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC87wkdFG3wp"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validation, y_validation, X_tst_bias2, y_tst_bias2):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier3 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier3.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias2\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred3 = classifier3.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred3))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred3))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred3, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred3, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred3))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias2\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred3_test = classifier3.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred3_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred3_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred3_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred3_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred3_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred3_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrD6SY0BG8kh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6GUYWY1G86g"
      },
      "source": [
        "**BIAS 3 - BASE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGH9CLXiG-0q"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# load the pretrained file GLove for word similairty\n",
        "glove_file = datapath('/content/drive/My Drive/Colab Notebooks/Thesis Project/Datasets GloVe/glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile('glove.6B.100d.txt')\n",
        "\n",
        "glove2word2vec(glove_file,word2vec_glove_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGbOf1M8HGB1"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fptWrqgHEUn"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3_20 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_20:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3_20.append(counter)\n",
        "\n",
        "X_val_biased_3_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias3_20[j]*item)\n",
        "\n",
        "    X_val_biased_3_20.append(temp_num_20)\n",
        "X_val_biased_3_20 = np.array(X_val_biased_3_20)\n",
        "\n",
        "X_test_biased_3_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias3_20[j]*item)\n",
        "\n",
        "    X_test_biased_3_20.append(temp_num_20)\n",
        "X_test_biased_3_20 = np.array(X_test_biased_3_20)\n",
        "\n",
        "X_val_bias3_20 = X_val_biased_3_20\n",
        "y_val_bias3_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3_20.append(class_made)\n",
        "\n",
        "y_val_bias3_20 = np.array(y_val_bias3_20)\n",
        "\n",
        "X_tst_bias3_20 = X_test_biased_3_20\n",
        "y_tst_bias3_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3_20.append(class_made)\n",
        "\n",
        "y_tst_bias3_20 = np.array(y_tst_bias3_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtqRnx24HR5T"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXLknlaMHTc2"
      },
      "source": [
        "# create biases in the same fashion as before\n",
        "X_bias3_50 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_50:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3_50.append(counter)\n",
        "\n",
        "X_val_biased_3_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias3_50[j]*item)\n",
        "\n",
        "    X_val_biased_3_50.append(temp_num_50)\n",
        "X_val_biased_3_50 = np.array(X_val_biased_3_50)\n",
        "\n",
        "X_test_biased_3_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias3_50[j]*item)\n",
        "\n",
        "    X_test_biased_3_50.append(temp_num_50)\n",
        "X_test_biased_3_50 = np.array(X_test_biased_3_50)\n",
        "\n",
        "X_val_bias3_50 = X_val_biased_3_50\n",
        "y_val_bias3_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3_50.append(class_made)\n",
        "\n",
        "y_val_bias3_50 = np.array(y_val_bias3_50)\n",
        "\n",
        "X_tst_bias3_50 = X_test_biased_3_50\n",
        "y_tst_bias3_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3_50.append(class_made)\n",
        "\n",
        "y_tst_bias3_50 = np.array(y_tst_bias3_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKVH-AioHboU"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJx58Wr5HdI2"
      },
      "source": [
        "# create biases in the same fashion as before\n",
        "X_bias3_100 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_100:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3_100.append(counter)\n",
        "\n",
        "X_val_biased_3_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias3_100[j]*item)\n",
        "\n",
        "    X_val_biased_3_100.append(temp_num_100)\n",
        "X_val_biased_3_100 = np.array(X_val_biased_3_100)\n",
        "\n",
        "X_test_biased_3_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias3_100[j]*item)\n",
        "\n",
        "    X_test_biased_3_100.append(temp_num_100)\n",
        "X_test_biased_3_100 = np.array(X_test_biased_3_100)\n",
        "\n",
        "X_val_bias3_100 = X_val_biased_3_100\n",
        "y_val_bias3_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3_100.append(class_made)\n",
        "\n",
        "y_val_bias3_100 = np.array(y_val_bias3_100)\n",
        "\n",
        "X_tst_bias3_100 = X_test_biased_3_100\n",
        "y_tst_bias3_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3_100.append(class_made)\n",
        "\n",
        "y_tst_bias3_100 = np.array(y_tst_bias3_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_GXJlDVHmdM"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVafy0i3HoFC"
      },
      "source": [
        "# create biases in the same fashion as before\n",
        "X_bias3_200 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_200:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3_200.append(counter)\n",
        "\n",
        "X_val_biased_3_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias3_200[j]*item)\n",
        "\n",
        "    X_val_biased_3_200.append(temp_num_200)\n",
        "X_val_biased_3_200 = np.array(X_val_biased_3_200)\n",
        "\n",
        "X_test_biased_3_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias3_200[j]*item)\n",
        "\n",
        "    X_test_biased_3_200.append(temp_num_200)\n",
        "X_test_biased_3_200 = np.array(X_test_biased_3_200)\n",
        "\n",
        "X_val_bias3_200 = X_val_biased_3_200\n",
        "y_val_bias3_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3_200.append(class_made)\n",
        "\n",
        "y_val_bias3_200 = np.array(y_val_bias3_200)\n",
        "\n",
        "X_tst_bias3_200 = X_test_biased_3_200\n",
        "y_tst_bias3_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3_200.append(class_made)\n",
        "\n",
        "y_tst_bias3_200 = np.array(y_tst_bias3_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXmUi75YHv4G"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG8pG3gAHxcF"
      },
      "source": [
        "# create biases in the same fashion as before\n",
        "X_bias3_500 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_500:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3_500.append(counter)\n",
        "\n",
        "X_val_biased_3_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias3_500[j]*item)\n",
        "\n",
        "    X_val_biased_3_500.append(temp_num_500)\n",
        "X_val_biased_3_500 = np.array(X_val_biased_3_500)\n",
        "\n",
        "X_test_biased_3_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias3_500[j]*item)\n",
        "\n",
        "    X_test_biased_3_500.append(temp_num_500)\n",
        "X_test_biased_3_500 = np.array(X_test_biased_3_500)\n",
        "\n",
        "X_val_bias3_500 = X_val_biased_3_500\n",
        "y_val_bias3_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3_500.append(class_made)\n",
        "\n",
        "y_val_bias3_500 = np.array(y_val_bias3_500)\n",
        "\n",
        "X_tst_bias3_500 = X_test_biased_3_500\n",
        "y_tst_bias3_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3_500.append(class_made)\n",
        "\n",
        "y_tst_bias3_500 = np.array(y_tst_bias3_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NHvq-a2H5JA"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwhqDe8gH7LY"
      },
      "source": [
        "# create biases in the same fashion as before\n",
        "X_bias3_750 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_750:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3_750.append(counter)\n",
        "\n",
        "X_val_biased_3_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias3_750[j]*item)\n",
        "\n",
        "    X_val_biased_3_750.append(temp_num_750)\n",
        "X_val_biased_3_750 = np.array(X_val_biased_3_750)\n",
        "\n",
        "X_test_biased_3_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias3_750[j]*item)\n",
        "\n",
        "    X_test_biased_3_750.append(temp_num_750)\n",
        "X_test_biased_3_750 = np.array(X_test_biased_3_750)\n",
        "\n",
        "X_val_bias3_750 = X_val_biased_3_750\n",
        "y_val_bias3_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3_750.append(class_made)\n",
        "\n",
        "y_val_bias3_750 = np.array(y_val_bias3_750)\n",
        "\n",
        "X_tst_bias3_750 = X_test_biased_3_750\n",
        "y_tst_bias3_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3_750.append(class_made)\n",
        "\n",
        "y_tst_bias3_750 = np.array(y_tst_bias3_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC4fgaeJIDJ0"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA2p6X0sIEvW"
      },
      "source": [
        "# create biases in the same fashion as before\n",
        "X_bias3_1000 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list1_1000:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3_1000.append(counter)\n",
        "\n",
        "X_val_biased_3_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias3_1000[j]*item)\n",
        "\n",
        "    X_val_biased_3_1000.append(temp_num_1000)\n",
        "X_val_biased_3_1000 = np.array(X_val_biased_3_1000)\n",
        "\n",
        "X_test_biased_3_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias3_1000[j]*item)\n",
        "\n",
        "    X_test_biased_3_1000.append(temp_num_1000)\n",
        "X_test_biased_3_1000 = np.array(X_test_biased_3_1000)\n",
        "\n",
        "X_val_bias3_1000 = X_val_biased_3_1000\n",
        "y_val_bias3_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3_1000.append(class_made)\n",
        "\n",
        "y_val_bias3_1000 = np.array(y_val_bias3_1000)\n",
        "\n",
        "X_tst_bias3_1000 = X_test_biased_3_1000\n",
        "y_tst_bias3_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3_1000.append(class_made)\n",
        "\n",
        "y_tst_bias3_1000 = np.array(y_tst_bias3_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gBy5GFxIMuj"
      },
      "source": [
        "## **SVM BIAS 3 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LveT3rA2IPBM"
      },
      "source": [
        "X_validation = X_val_bias3_20, X_val_bias3_50, X_val_bias3_100, X_val_bias3_200, X_val_bias3_500, X_val_bias3_750, X_val_bias3_1000\n",
        "y_validation = y_val_bias3_20, y_val_bias3_50, y_val_bias3_100, y_val_bias3_200, y_val_bias3_500, y_val_bias3_750, y_val_bias3_1000\n",
        "\n",
        "X_tst_bias3 = X_tst_bias3_20, X_tst_bias3_50, X_tst_bias3_100, X_tst_bias3_200, X_tst_bias3_500, X_tst_bias3_750, X_tst_bias3_1000\n",
        "y_tst_bias3 = y_tst_bias3_20, y_tst_bias3_50, y_tst_bias3_100, y_tst_bias3_200, y_tst_bias3_500, y_tst_bias3_750, y_tst_bias3_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iabk9DcZIQbK"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validation, y_validation, X_tst_bias3, y_tst_bias3):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier4 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier4.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias3\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred4 = classifier4.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred4))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred4))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred4, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred4, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred4))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias3\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred4_test = classifier4.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred4_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred4_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred4_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred4_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred4_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred4_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eyPV2TUITWJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFi4B0hzITpJ"
      },
      "source": [
        "**BIAS 4 - BASE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gc59iLzIZfI"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZmnXc1IWRI"
      },
      "source": [
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# now for the last bias\n",
        "X_bias4_20 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_20:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4_20.append(counter)\n",
        "\n",
        "X_val_biased_4_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias4_20[j]*item)\n",
        "\n",
        "    X_val_biased_4_20.append(temp_num_20)\n",
        "X_val_biased_4_20 = np.array(X_val_biased_4_20)\n",
        "\n",
        "X_test_biased_4_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias4_20[j]*item)\n",
        "\n",
        "    X_test_biased_4_20.append(temp_num_20)\n",
        "X_test_biased_4_20 = np.array(X_test_biased_4_20)\n",
        "\n",
        "X_val_bias4_20 = X_val_biased_4_20\n",
        "y_val_bias4_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4_20.append(class_made)\n",
        "\n",
        "y_val_bias4_20 = np.array(y_val_bias4_20)\n",
        "\n",
        "X_tst_bias4_20 = X_test_biased_4_20\n",
        "y_tst_bias4_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4_20.append(class_made)\n",
        "\n",
        "y_tst_bias4_20 = np.array(y_tst_bias4_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-QNyxf6Ik3e"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KunbzIslIpEB"
      },
      "source": [
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# now for the last bias\n",
        "X_bias4_50 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_50:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4_50.append(counter)\n",
        "\n",
        "X_val_biased_4_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias4_50[j]*item)\n",
        "\n",
        "    X_val_biased_4_50.append(temp_num_50)\n",
        "X_val_biased_4_50 = np.array(X_val_biased_4_50)\n",
        "\n",
        "X_test_biased_4_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias4_50[j]*item)\n",
        "\n",
        "    X_test_biased_4_50.append(temp_num_50)\n",
        "X_test_biased_4_50 = np.array(X_test_biased_4_50)\n",
        "\n",
        "X_val_bias4_50 = X_val_biased_4_50\n",
        "y_val_bias4_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4_50.append(class_made)\n",
        "\n",
        "y_val_bias4_50 = np.array(y_val_bias4_50)\n",
        "\n",
        "X_tst_bias4_50 = X_test_biased_4_50\n",
        "y_tst_bias4_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4_50.append(class_made)\n",
        "\n",
        "y_tst_bias4_50 = np.array(y_tst_bias4_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu374p1MIycN"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae9TIpvtIzya"
      },
      "source": [
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# now for the last bias\n",
        "X_bias4_100 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_100:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4_100.append(counter)\n",
        "\n",
        "X_val_biased_4_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias4_100[j]*item)\n",
        "\n",
        "    X_val_biased_4_100.append(temp_num_100)\n",
        "X_val_biased_4_100 = np.array(X_val_biased_4_100)\n",
        "\n",
        "X_test_biased_4_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias4_100[j]*item)\n",
        "\n",
        "    X_test_biased_4_100.append(temp_num_100)\n",
        "X_test_biased_4_100 = np.array(X_test_biased_4_100)\n",
        "\n",
        "X_val_bias4_100 = X_val_biased_4_100\n",
        "y_val_bias4_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4_100.append(class_made)\n",
        "\n",
        "y_val_bias4_100 = np.array(y_val_bias4_100)\n",
        "\n",
        "X_tst_bias4_100 = X_test_biased_4_100\n",
        "y_tst_bias4_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4_100.append(class_made)\n",
        "\n",
        "y_tst_bias4_100 = np.array(y_tst_bias4_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNdgZ42rI9TO"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr5V0YQGI-t6"
      },
      "source": [
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# now for the last bias\n",
        "X_bias4_200 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_200:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4_200.append(counter)\n",
        "\n",
        "X_val_biased_4_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias4_200[j]*item)\n",
        "\n",
        "    X_val_biased_4_200.append(temp_num_200)\n",
        "X_val_biased_4_200 = np.array(X_val_biased_4_200)\n",
        "\n",
        "X_test_biased_4_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias4_200[j]*item)\n",
        "\n",
        "    X_test_biased_4_200.append(temp_num_200)\n",
        "X_test_biased_4_200 = np.array(X_test_biased_4_200)\n",
        "\n",
        "X_val_bias4_200 = X_val_biased_4_200\n",
        "y_val_bias4_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4_200.append(class_made)\n",
        "\n",
        "y_val_bias4_200 = np.array(y_val_bias4_200)\n",
        "\n",
        "X_tst_bias4_200 = X_test_biased_4_200\n",
        "y_tst_bias4_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4_200.append(class_made)\n",
        "\n",
        "y_tst_bias4_200 = np.array(y_tst_bias4_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLiaP6B3JLlx"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axtJyg3hJNHu"
      },
      "source": [
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# now for the last bias\n",
        "X_bias4_500 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_500:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4_500.append(counter)\n",
        "\n",
        "X_val_biased_4_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias4_500[j]*item)\n",
        "\n",
        "    X_val_biased_4_500.append(temp_num_500)\n",
        "X_val_biased_4_500 = np.array(X_val_biased_4_500)\n",
        "\n",
        "X_test_biased_4_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias4_500[j]*item)\n",
        "\n",
        "    X_test_biased_4_500.append(temp_num_500)\n",
        "X_test_biased_4_500 = np.array(X_test_biased_4_500)\n",
        "\n",
        "X_val_bias4_500 = X_val_biased_4_500\n",
        "y_val_bias4_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4_500.append(class_made)\n",
        "\n",
        "y_val_bias4_500 = np.array(y_val_bias4_500)\n",
        "\n",
        "X_tst_bias4_500 = X_test_biased_4_500\n",
        "y_tst_bias4_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4_500.append(class_made)\n",
        "\n",
        "y_tst_bias4_500 = np.array(y_tst_bias4_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-iOqL4IJX1z"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTjT1N_BJZci"
      },
      "source": [
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# now for the last bias\n",
        "X_bias4_750 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_750:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4_750.append(counter)\n",
        "\n",
        "X_val_biased_4_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias4_750[j]*item)\n",
        "\n",
        "    X_val_biased_4_750.append(temp_num_750)\n",
        "X_val_biased_4_750 = np.array(X_val_biased_4_750)\n",
        "\n",
        "X_test_biased_4_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias4_750[j]*item)\n",
        "\n",
        "    X_test_biased_4_750.append(temp_num_750)\n",
        "X_test_biased_4_750 = np.array(X_test_biased_4_750)\n",
        "\n",
        "X_val_bias4_750 = X_val_biased_4_750\n",
        "y_val_bias4_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4_750.append(class_made)\n",
        "\n",
        "y_val_bias4_750 = np.array(y_val_bias4_750)\n",
        "\n",
        "X_tst_bias4_750 = X_test_biased_4_750\n",
        "y_tst_bias4_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4_750.append(class_made)\n",
        "\n",
        "y_tst_bias4_750 = np.array(y_tst_bias4_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCOTFt1lJh3p"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ_XDADUJi3P"
      },
      "source": [
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# now for the last bias\n",
        "X_bias4_1000 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in keyword_list2_1000:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4_1000.append(counter)\n",
        "\n",
        "X_val_biased_4_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias4_1000[j]*item)\n",
        "\n",
        "    X_val_biased_4_1000.append(temp_num_1000)\n",
        "X_val_biased_4_1000 = np.array(X_val_biased_4_1000)\n",
        "\n",
        "X_test_biased_4_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias4_1000[j]*item)\n",
        "\n",
        "    X_test_biased_4_1000.append(temp_num_1000)\n",
        "X_test_biased_4_1000 = np.array(X_test_biased_4_1000)\n",
        "\n",
        "X_val_bias4_1000 = X_val_biased_4_1000\n",
        "y_val_bias4_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4_1000.append(class_made)\n",
        "\n",
        "y_val_bias4_1000 = np.array(y_val_bias4_1000)\n",
        "\n",
        "X_tst_bias4_1000 = X_test_biased_4_1000\n",
        "y_tst_bias4_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4_1000.append(class_made)\n",
        "\n",
        "y_tst_bias4_1000 = np.array(y_tst_bias4_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKnDaMvDJzFt"
      },
      "source": [
        "## **SVM BIAS 4 - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmKPDST-J11q"
      },
      "source": [
        "X_validation = X_val_bias4_20, X_val_bias4_50, X_val_bias4_100, X_val_bias4_200, X_val_bias4_500, X_val_bias4_750, X_val_bias4_1000\n",
        "y_validation = y_val_bias4_20, y_val_bias4_50, y_val_bias4_100, y_val_bias4_200, y_val_bias4_500, y_val_bias4_750, y_val_bias4_1000\n",
        "\n",
        "X_tst_bias4 = X_tst_bias4_20, X_tst_bias4_50, X_tst_bias4_100, X_tst_bias4_200, X_tst_bias4_500, X_tst_bias4_750, X_tst_bias4_1000\n",
        "y_tst_bias4 = y_tst_bias4_20, y_tst_bias4_50, y_tst_bias4_100, y_tst_bias4_200, y_tst_bias4_500, y_tst_bias4_750, y_tst_bias4_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ohnq8JjJ3TM"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validation, y_validation, X_tst_bias4, y_tst_bias4):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier6 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier6.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias4\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred5 = classifier6.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred5))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred5))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred5, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred5, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred5))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias4\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred5_test = classifier6.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred5_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred5_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred5_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred5_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred5_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred5_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaE5ZBm4J5tw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0HVQ5bxJ5_p"
      },
      "source": [
        "# NER BIASED CLASSIFIERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXJAuZVlKh4b"
      },
      "source": [
        "**BIAS 1 - NER (CoNLL-2003)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXYesx1_KkZR"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5FUzvVLKno9"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1c_20 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_20:\n",
        "          try:\n",
        "              counter += model.wv.similarity(w1, w2)\n",
        "              averager += 1\n",
        "          except:\n",
        "              averager -= 1\n",
        "              pass\n",
        "\n",
        "        \n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1c_20.append(counter)\n",
        "\n",
        "X_val_biased_1c_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias1c_20[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_1c_20.append(temp_num_20)\n",
        "X_val_biased_1c_20 = np.array(X_val_biased_1c_20)\n",
        "\n",
        "X_test_biased_1c_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias1c_20[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_1c_20.append(temp_num_20)\n",
        "X_test_biased_1c_20 = np.array(X_test_biased_1c_20)\n",
        "\n",
        "X_val_bias1c_20 = X_val_biased_1c_20\n",
        "y_val_bias1c_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1c_20.append(class_made)\n",
        "\n",
        "y_val_bias1c_20 = np.array(y_val_bias1c_20)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1c_20 = X_test_biased_1c_20\n",
        "y_tst_bias1c_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1c_20.append(class_made)\n",
        "\n",
        "y_tst_bias1c_20 = np.array(y_tst_bias1c_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlEcsLnOLCWF"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbA0YflyLEUs"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1c_50 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "       \n",
        "        for w2 in ner_keyword_list1_50:\n",
        "          try:\n",
        "            counter += model.wv.similarity(w1, w2)\n",
        "            averager += 1\n",
        "          except: \n",
        "            averager -= 1\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1c_50.append(counter)\n",
        "\n",
        "X_val_biased_1c_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias1c_50[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_1c_50.append(temp_num_50)\n",
        "X_val_biased_1c_50 = np.array(X_val_biased_1c_50)\n",
        "\n",
        "X_test_biased_1c_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias1c_50[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_1c_50.append(temp_num_50)\n",
        "X_test_biased_1c_50 = np.array(X_test_biased_1c_50)\n",
        "\n",
        "X_val_bias1c_50 = X_val_biased_1c_50\n",
        "y_val_bias1c_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1c_50.append(class_made)\n",
        "\n",
        "y_val_bias1c_50 = np.array(y_val_bias1c_50)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1c_50 = X_test_biased_1c_50\n",
        "y_tst_bias1c_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1c_50.append(class_made)\n",
        "\n",
        "y_tst_bias1c_50 = np.array(y_tst_bias1c_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb94sc1fLXje"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsWeiTQULYeZ"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1c_100 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "\n",
        "        for w2 in ner_keyword_list1_100:\n",
        "          try:\n",
        "            counter += model.wv.similarity(w1, w2)\n",
        "            averager += 1\n",
        "          except: \n",
        "            averager -= 1\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1c_100.append(counter)\n",
        "\n",
        "X_val_biased_1c_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias1c_100[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_1c_100.append(temp_num_100)\n",
        "X_val_biased_1c_100 = np.array(X_val_biased_1c_100)\n",
        "\n",
        "X_test_biased_1c_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias1c_100[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_1c_100.append(temp_num_100)\n",
        "X_test_biased_1c_100 = np.array(X_test_biased_1c_100)\n",
        "\n",
        "X_val_bias1c_100 = X_val_biased_1c_100\n",
        "y_val_bias1c_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1c_100.append(class_made)\n",
        "\n",
        "y_val_bias1c_100 = np.array(y_val_bias1c_100)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1c_100 = X_test_biased_1c_100\n",
        "y_tst_bias1c_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1c_100.append(class_made)\n",
        "\n",
        "y_tst_bias1c_100 = np.array(y_tst_bias1c_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdFSxLNdLjK1"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOpo3K7NLkfB"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1c_200 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "\n",
        "        for w2 in ner_keyword_list1_200:\n",
        "          try:\n",
        "            counter += model.wv.similarity(w1, w2)\n",
        "            averager += 1\n",
        "          except: \n",
        "            averager -= 1\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1c_200.append(counter)\n",
        "\n",
        "X_val_biased_1c_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias1c_200[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_1c_200.append(temp_num_200)\n",
        "X_val_biased_1c_200 = np.array(X_val_biased_1c_200)\n",
        "\n",
        "X_test_biased_1c_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias1c_200[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_1c_200.append(temp_num_200)\n",
        "X_test_biased_1c_200 = np.array(X_test_biased_1c_200)\n",
        "\n",
        "X_val_bias1c_200 = X_val_biased_1c_200\n",
        "y_val_bias1c_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1c_200.append(class_made)\n",
        "\n",
        "y_val_bias1c_200 = np.array(y_val_bias1c_200)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1c_200 = X_test_biased_1c_200\n",
        "y_tst_bias1c_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1c_200.append(class_made)\n",
        "\n",
        "y_tst_bias1c_200 = np.array(y_tst_bias1c_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TumDff5jLwDb"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge1kZufYLxqk"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1c_500 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "\n",
        "        for w2 in ner_keyword_list1_500:\n",
        "          try:\n",
        "            counter += model.wv.similarity(w1, w2)\n",
        "            averager += 1\n",
        "          except: \n",
        "            averager -= 1\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1c_500.append(counter)\n",
        "\n",
        "X_val_biased_1c_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias1c_500[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_1c_500.append(temp_num_500)\n",
        "X_val_biased_1c_500 = np.array(X_val_biased_1c_500)\n",
        "\n",
        "X_test_biased_1c_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias1c_500[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_1c_500.append(temp_num_500)\n",
        "X_test_biased_1c_500 = np.array(X_test_biased_1c_500)\n",
        "\n",
        "X_val_bias1c_500 = X_val_biased_1c_500\n",
        "y_val_bias1c_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1c_500.append(class_made)\n",
        "\n",
        "y_val_bias1c_500 = np.array(y_val_bias1c_500)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1c_500 = X_test_biased_1c_500\n",
        "y_tst_bias1c_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1c_500.append(class_made)\n",
        "\n",
        "y_tst_bias1c_500 = np.array(y_tst_bias1c_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPQbsgICL8IF"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f6BEzYhL9mQ"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1c_750 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "\n",
        "        for w2 in ner_keyword_list1_750:\n",
        "          try:\n",
        "            counter += model.wv.similarity(w1, w2)\n",
        "            averager += 1\n",
        "          except: \n",
        "            averager -= 1\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1c_750.append(counter)\n",
        "\n",
        "X_val_biased_1c_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias1c_750[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_1c_750.append(temp_num_750)\n",
        "X_val_biased_1c_750 = np.array(X_val_biased_1c_750)\n",
        "\n",
        "X_test_biased_1c_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias1c_750[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_1c_750.append(temp_num_750)\n",
        "X_test_biased_1c_750 = np.array(X_test_biased_1c_750)\n",
        "\n",
        "X_val_bias1c_750 = X_val_biased_1c_750\n",
        "y_val_bias1c_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1c_750.append(class_made)\n",
        "\n",
        "y_val_bias1c_750 = np.array(y_val_bias1c_750)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1c_750 = X_test_biased_1c_750\n",
        "y_tst_bias1c_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1c_750.append(class_made)\n",
        "\n",
        "y_tst_bias1c_750 = np.array(y_tst_bias1c_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqJALXVoMI1k"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2CzToeZMKcf"
      },
      "source": [
        "# here I start by calculating our bias multiplier matrix\n",
        "X_bias1c_1000 = []\n",
        "\n",
        "# initialise and train the Word2Vec model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "\n",
        "        for w2 in ner_keyword_list1_1000:\n",
        "          try:\n",
        "            counter += model.wv.similarity(w1, w2)\n",
        "            averager += 1\n",
        "          except: \n",
        "            averager -= 1\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias1c_1000.append(counter)\n",
        "\n",
        "X_val_biased_1c_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias1c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_1c_1000.append(temp_num_1000)\n",
        "X_val_biased_1c_1000 = np.array(X_val_biased_1c_1000)\n",
        "\n",
        "X_test_biased_1c_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias1c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_1c_1000.append(temp_num_1000)\n",
        "X_test_biased_1c_1000 = np.array(X_test_biased_1c_1000)\n",
        "\n",
        "X_val_bias1c_1000 = X_val_biased_1c_1000\n",
        "y_val_bias1c_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias1c_1000.append(class_made)\n",
        "\n",
        "y_val_bias1c_1000 = np.array(y_val_bias1c_1000)\n",
        "\n",
        "# for testing\n",
        "X_tst_bias1c_1000 = X_test_biased_1c_1000\n",
        "y_tst_bias1c_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias1c_1000.append(class_made)\n",
        "\n",
        "y_tst_bias1c_1000 = np.array(y_tst_bias1c_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz6ozGy_MWnv"
      },
      "source": [
        "## **SVM BIAS 1 NER - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEW7nqV7MZpz"
      },
      "source": [
        "X_validation = X_val_bias1c_20, X_val_bias1c_50, X_val_bias1c_100, X_val_bias1c_200, X_val_bias1c_500, X_val_bias1c_750, X_val_bias1c_1000\n",
        "y_validation = y_val_bias1c_20, y_val_bias1c_50, y_val_bias1c_100, y_val_bias1c_200, y_val_bias1c_500, y_val_bias1c_750, y_val_bias1c_1000\n",
        "\n",
        "X_tst_bias1c = X_tst_bias1c_20, X_tst_bias1c_50, X_tst_bias1c_100, X_tst_bias1c_200, X_tst_bias1c_500, X_tst_bias1c_750, X_tst_bias1c_1000\n",
        "y_tst_bias1c = y_tst_bias1c_20, y_tst_bias1c_50, y_tst_bias1c_100, y_tst_bias1c_200, y_tst_bias1c_500, y_tst_bias1c_750, y_tst_bias1c_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axnxANrCMbwv"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validation, y_validation, X_tst_bias1c, y_tst_bias1c):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier10 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier10.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias1 - NER\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred2 = classifier10.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred2))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred2))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred2, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred2, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred2))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias1 - NER\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred2_test = classifier10.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred2_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred2_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred2_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred2_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred2_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred2_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-SyqtGIMduc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9utzNqksMd9s"
      },
      "source": [
        "**BIAS 2 - NER (CoNLL-2003)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFU2pmBiMp6_"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-aQKJSVMlbL"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2c_20 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_20:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2c_20.append(counter)\n",
        "\n",
        "X_val_biased_2c_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias2c_20[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_2c_20.append(temp_num_20)\n",
        "X_val_biased_2c_20 = np.array(X_val_biased_2c_20)\n",
        "\n",
        "X_test_biased_2c_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias2c_20[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_2c_20.append(temp_num_20)\n",
        "X_test_biased_2c_20 = np.array(X_test_biased_2c_20)\n",
        "\n",
        "X_val_bias2c_20 = X_val_biased_2c_20\n",
        "y_val_bias2c_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2c_20.append(class_made)\n",
        "\n",
        "y_val_bias2c_20 = np.array(y_val_bias2c_20)\n",
        "\n",
        "X_tst_bias2c_20 = X_test_biased_2c_20\n",
        "y_tst_bias2c_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2c_20.append(class_made)\n",
        "\n",
        "y_tst_bias2c_20 = np.array(y_tst_bias2c_20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkg3aM6KMs9j"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkcnnAh4Mucb"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2c_50 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_50:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2c_50.append(counter)\n",
        "\n",
        "X_val_biased_2c_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias2c_50[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_2c_50.append(temp_num_50)\n",
        "X_val_biased_2c_50 = np.array(X_val_biased_2c_50)\n",
        "\n",
        "X_test_biased_2c_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias2c_50[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_2c_50.append(temp_num_50)\n",
        "X_test_biased_2c_50 = np.array(X_test_biased_2c_50)\n",
        "\n",
        "X_val_bias2c_50 = X_val_biased_2c_50\n",
        "y_val_bias2c_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2c_50.append(class_made)\n",
        "\n",
        "y_val_bias2c_50 = np.array(y_val_bias2c_50)\n",
        "\n",
        "X_tst_bias2c_50 = X_test_biased_2c_50\n",
        "y_tst_bias2c_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2c_50.append(class_made)\n",
        "\n",
        "y_tst_bias2c_50 = np.array(y_tst_bias2c_50)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-MoidXPMwor"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFI_3gZMMyoa"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2c_100 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_100:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2c_100.append(counter)\n",
        "\n",
        "X_val_biased_2c_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias2c_100[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_2c_100.append(temp_num_100)\n",
        "X_val_biased_2c_100 = np.array(X_val_biased_2c_100)\n",
        "\n",
        "X_test_biased_2c_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias2c_100[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_2c_100.append(temp_num_100)\n",
        "X_test_biased_2c_100 = np.array(X_test_biased_2c_100)\n",
        "\n",
        "X_val_bias2c_100 = X_val_biased_2c_100\n",
        "y_val_bias2c_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2c_100.append(class_made)\n",
        "\n",
        "y_val_bias2c_100 = np.array(y_val_bias2c_100)\n",
        "\n",
        "X_tst_bias2c_100 = X_test_biased_2c_100\n",
        "y_tst_bias2c_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2c_100.append(class_made)\n",
        "\n",
        "y_tst_bias2c_100 = np.array(y_tst_bias2c_100)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDNFXEepM0aC"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQAigELfM2eR"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2c_200 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_200:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2c_200.append(counter)\n",
        "\n",
        "X_val_biased_2c_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias2c_200[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_2c_200.append(temp_num_200)\n",
        "X_val_biased_2c_200 = np.array(X_val_biased_2c_200)\n",
        "\n",
        "X_test_biased_2c_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias2c_200[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_2c_200.append(temp_num_200)\n",
        "X_test_biased_2c_200 = np.array(X_test_biased_2c_200)\n",
        "\n",
        "X_val_bias2c_200 = X_val_biased_2c_200\n",
        "y_val_bias2c_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2c_200.append(class_made)\n",
        "\n",
        "y_val_bias2c_200 = np.array(y_val_bias2c_200)\n",
        "\n",
        "X_tst_bias2c_200 = X_test_biased_2c_200\n",
        "y_tst_bias2c_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2c_200.append(class_made)\n",
        "\n",
        "y_tst_bias2c_200 = np.array(y_tst_bias2c_200)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942X780wM4py"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrM4BwP8M6Pq"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2c_500 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_500:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2c_500.append(counter)\n",
        "\n",
        "X_val_biased_2c_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias2c_500[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_2c_500.append(temp_num_500)\n",
        "X_val_biased_2c_500 = np.array(X_val_biased_2c_500)\n",
        "\n",
        "X_test_biased_2c_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias2c_500[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_2c_500.append(temp_num_500)\n",
        "X_test_biased_2c_500 = np.array(X_test_biased_2c_500)\n",
        "\n",
        "X_val_bias2c_500 = X_val_biased_2c_500\n",
        "y_val_bias2c_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2c_500.append(class_made)\n",
        "\n",
        "y_val_bias2c_500 = np.array(y_val_bias2c_500)\n",
        "\n",
        "X_tst_bias2c_500 = X_test_biased_2c_500\n",
        "y_tst_bias2c_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2c_500.append(class_made)\n",
        "\n",
        "y_tst_bias2c_500 = np.array(y_tst_bias2c_500)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OemIFiOM8NX"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7ZcYHyYM97M"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2c_750 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_750:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2c_750.append(counter)\n",
        "\n",
        "X_val_biased_2c_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias2c_750[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_2c_750.append(temp_num_750)\n",
        "X_val_biased_2c_750 = np.array(X_val_biased_2c_750)\n",
        "\n",
        "X_test_biased_2c_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias2c_750[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_2c_750.append(temp_num_750)\n",
        "X_test_biased_2c_750 = np.array(X_test_biased_2c_750)\n",
        "\n",
        "X_val_bias2c_750 = X_val_biased_2c_750\n",
        "y_val_bias2c_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2c_750.append(class_made)\n",
        "\n",
        "y_val_bias2c_750 = np.array(y_val_bias2c_750)\n",
        "\n",
        "X_tst_bias2c_750 = X_test_biased_2c_750\n",
        "y_tst_bias2c_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2c_750.append(class_made)\n",
        "\n",
        "y_tst_bias2c_750 = np.array(y_tst_bias2c_750)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0mmNdjZM_qw"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv3P711jNBOh"
      },
      "source": [
        "# now to bias the X for bias 2\n",
        "X_bias2c_1000 = []\n",
        "\n",
        "# initialise and train the model\n",
        "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=10, iter=10)\n",
        "\n",
        "# create a list with bias multipliers for each of the words in the TF-IDF vocab, now with wordlist 2\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_1000:\n",
        "            try:\n",
        "                counter += model.wv.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "          counter = counter/averager\n",
        "        except:\n",
        "          counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias2c_1000.append(counter)\n",
        "\n",
        "X_val_biased_2c_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias2c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_2c_1000.append(temp_num_1000)\n",
        "X_val_biased_2c_1000 = np.array(X_val_biased_2c_1000)\n",
        "\n",
        "X_test_biased_2c_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias2c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_2c_1000.append(temp_num_1000)\n",
        "X_test_biased_2c_1000 = np.array(X_test_biased_2c_1000)\n",
        "\n",
        "X_val_bias2c_1000 = X_val_biased_2c_1000\n",
        "y_val_bias2c_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias2c_1000.append(class_made)\n",
        "\n",
        "y_val_bias2c_1000 = np.array(y_val_bias2c_1000)\n",
        "\n",
        "X_tst_bias2c_1000 = X_test_biased_2c_1000\n",
        "y_tst_bias2c_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias2c_1000.append(class_made)\n",
        "\n",
        "y_tst_bias2c_1000 = np.array(y_tst_bias2c_1000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S9gyGSgNGkE"
      },
      "source": [
        "## **SVM BIAS 2 NER - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msI9ZD4BNHAh"
      },
      "source": [
        "X_validation = X_val_bias2c_20, X_val_bias2c_50, X_val_bias2c_100, X_val_bias2c_200, X_val_bias2c_500, X_val_bias2c_750, X_val_bias2c_1000\n",
        "y_validation = y_val_bias2c_20, y_val_bias2c_50, y_val_bias2c_100, y_val_bias2c_200, y_val_bias2c_500, y_val_bias2c_750, y_val_bias2c_1000\n",
        "\n",
        "X_tst_bias2 = X_tst_bias2c_20, X_tst_bias2c_50, X_tst_bias2c_100, X_tst_bias2c_200, X_tst_bias2c_500, X_tst_bias2c_750, X_tst_bias2c_1000\n",
        "y_tst_bias2 = y_tst_bias2c_20, y_tst_bias2c_50, y_tst_bias2c_100, y_tst_bias2c_200, y_tst_bias2c_500, y_tst_bias2c_750, y_tst_bias2c_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4ryhkQCNIa5"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validation, y_validation, X_tst_bias2, y_tst_bias2):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier3 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier3.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias2\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred3 = classifier3.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred3))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred3))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred3, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred3, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred3))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias2\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred3_test = classifier3.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred3_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred3_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred3_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred3_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred3_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred3_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENVOL9zNK1F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEA7BhOeNLMI"
      },
      "source": [
        "**BIAS 3 - NER (CoNLL-2003)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjtOnVYKNU_n"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# load the pretrained file GLove for word similairty\n",
        "glove_file = datapath('/content/drive/My Drive/Colab Notebooks/Thesis Project/Datasets GloVe/glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile('glove.6B.100d.txt')\n",
        "\n",
        "glove2word2vec(glove_file,word2vec_glove_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-_ZT0NGNPFs"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5qakTl-NQZt"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3c_20 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_20:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3c_20.append(counter)\n",
        "\n",
        "X_val_biased_3c_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias3c_20[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_3c_20.append(temp_num_20)\n",
        "X_val_biased_3c_20 = np.array(X_val_biased_3c_20)\n",
        "\n",
        "X_test_biased_3c_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias3c_20[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_3c_20.append(temp_num_20)\n",
        "X_test_biased_3c_20 = np.array(X_test_biased_3c_20)\n",
        "\n",
        "X_val_bias3c_20 = X_val_biased_3c_20\n",
        "y_val_bias3c_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3c_20.append(class_made)\n",
        "\n",
        "y_val_bias3c_20 = np.array(y_val_bias3c_20)\n",
        "\n",
        "X_tst_bias3c_20 = X_test_biased_3c_20\n",
        "y_tst_bias3c_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3c_20.append(class_made)\n",
        "\n",
        "y_tst_bias3c_20 = np.array(y_tst_bias3c_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nQk9sW0NdAE"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0SXBjUkNekY"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3c_50 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_50:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3c_50.append(counter)\n",
        "\n",
        "X_val_biased_3c_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias3c_50[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_3c_50.append(temp_num_50)\n",
        "X_val_biased_3c_50 = np.array(X_val_biased_3c_50)\n",
        "\n",
        "X_test_biased_3c_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias3c_50[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_3c_50.append(temp_num_50)\n",
        "X_test_biased_3c_50 = np.array(X_test_biased_3c_50)\n",
        "\n",
        "X_val_bias3c_50 = X_val_biased_3c_50\n",
        "y_val_bias3c_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3c_50.append(class_made)\n",
        "\n",
        "y_val_bias3c_50 = np.array(y_val_bias3c_50)\n",
        "\n",
        "X_tst_bias3c_50 = X_test_biased_3c_50\n",
        "y_tst_bias3c_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3c_50.append(class_made)\n",
        "\n",
        "y_tst_bias3c_50 = np.array(y_tst_bias3c_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HTTyJMNggT"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VFAfuylNh0f"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3c_100 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_100:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3c_100.append(counter)\n",
        "\n",
        "X_val_biased_3c_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias3c_100[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_3c_100.append(temp_num_100)\n",
        "X_val_biased_3c_100 = np.array(X_val_biased_3c_100)\n",
        "\n",
        "X_test_biased_3c_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias3c_100[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_3c_100.append(temp_num_100)\n",
        "X_test_biased_3c_100 = np.array(X_test_biased_3c_100)\n",
        "\n",
        "X_val_bias3c_100 = X_val_biased_3c_100\n",
        "y_val_bias3c_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3c_100.append(class_made)\n",
        "\n",
        "y_val_bias3c_100 = np.array(y_val_bias3c_100)\n",
        "\n",
        "X_tst_bias3c_100 = X_test_biased_3c_100\n",
        "y_tst_bias3c_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3c_100.append(class_made)\n",
        "\n",
        "y_tst_bias3c_100 = np.array(y_tst_bias3c_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiHnfSntNjqp"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5JBhnvzNlMf"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3c_200 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_200:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3c_200.append(counter)\n",
        "\n",
        "X_val_biased_3c_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias3c_200[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_3c_200.append(temp_num_200)\n",
        "X_val_biased_3c_200 = np.array(X_val_biased_3c_200)\n",
        "\n",
        "X_test_biased_3c_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias3c_200[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_3c_200.append(temp_num_200)\n",
        "X_test_biased_3c_200 = np.array(X_test_biased_3c_200)\n",
        "\n",
        "X_val_bias3c_200 = X_val_biased_3c_200\n",
        "y_val_bias3c_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3c_200.append(class_made)\n",
        "\n",
        "y_val_bias3c_200 = np.array(y_val_bias3c_200)\n",
        "\n",
        "X_tst_bias3c_200 = X_test_biased_3c_200\n",
        "y_tst_bias3c_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3c_200.append(class_made)\n",
        "\n",
        "y_tst_bias3c_200 = np.array(y_tst_bias3c_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI51HnvyNm-X"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flre96s_NoiI"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3c_500 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_500:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3c_500.append(counter)\n",
        "\n",
        "X_val_biased_3c_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias3c_500[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_3c_500.append(temp_num_500)\n",
        "X_val_biased_3c_500 = np.array(X_val_biased_3c_500)\n",
        "\n",
        "X_test_biased_3c_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias3c_500[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_3c_500.append(temp_num_500)\n",
        "X_test_biased_3c_500 = np.array(X_test_biased_3c_500)\n",
        "\n",
        "X_val_bias3c_500 = X_val_biased_3c_500\n",
        "y_val_bias3c_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3c_500.append(class_made)\n",
        "\n",
        "y_val_bias3c_500 = np.array(y_val_bias3c_500)\n",
        "\n",
        "X_tst_bias3c_500 = X_test_biased_3c_500\n",
        "y_tst_bias3c_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3c_500.append(class_made)\n",
        "\n",
        "y_tst_bias3c_500 = np.array(y_tst_bias3c_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiAvPirkNqpN"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMbH1n86NsCw"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3c_750 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_750:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3c_750.append(counter)\n",
        "\n",
        "X_val_biased_3c_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias3c_750[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_3c_750.append(temp_num_750)\n",
        "X_val_biased_3c_750 = np.array(X_val_biased_3c_750)\n",
        "\n",
        "X_test_biased_3c_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias3c_750[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_3c_750.append(temp_num_750)\n",
        "X_test_biased_3c_750 = np.array(X_test_biased_3c_750)\n",
        "\n",
        "X_val_bias3c_750 = X_val_biased_3c_750\n",
        "y_val_bias3c_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3c_750.append(class_made)\n",
        "\n",
        "y_val_bias3c_750 = np.array(y_val_bias3c_750)\n",
        "\n",
        "X_tst_bias3c_750 = X_test_biased_3c_750\n",
        "y_tst_bias3c_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3c_750.append(class_made)\n",
        "\n",
        "y_tst_bias3c_750 = np.array(y_tst_bias3c_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bu5W-BtNt4T"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn_dwt0oNvLf"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias3c_1000 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list1_1000:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias3c_1000.append(counter)\n",
        "\n",
        "X_val_biased_3c_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias3c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_3c_1000.append(temp_num_1000)\n",
        "X_val_biased_3c_1000 = np.array(X_val_biased_3c_1000)\n",
        "\n",
        "X_test_biased_3c_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias3c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_3c_1000.append(temp_num_1000)\n",
        "X_test_biased_3c_1000 = np.array(X_test_biased_3c_1000)\n",
        "\n",
        "X_val_bias3c_1000 = X_val_biased_3c_1000\n",
        "y_val_bias3c_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias3c_1000.append(class_made)\n",
        "\n",
        "y_val_bias3c_1000 = np.array(y_val_bias3c_1000)\n",
        "\n",
        "X_tst_bias3c_1000 = X_test_biased_3c_1000\n",
        "y_tst_bias3c_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias3c_1000.append(class_made)\n",
        "\n",
        "y_tst_bias3c_1000 = np.array(y_tst_bias3c_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5eqJs-rNxQV"
      },
      "source": [
        "## **SVM BIAS 3 NER - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owXBZYjsNzqo"
      },
      "source": [
        "X_validationc = X_val_bias3c_20, X_val_bias3c_50, X_val_bias3c_100, X_val_bias3c_200, X_val_bias3c_500, X_val_bias3c_750, X_val_bias3c_1000\n",
        "y_validationc = y_val_bias3c_20, y_val_bias3c_50, y_val_bias3c_100, y_val_bias3c_200, y_val_bias3c_500, y_val_bias3c_750, y_val_bias3c_1000\n",
        "\n",
        "X_tst_bias3c = X_tst_bias3c_20, X_tst_bias3c_50, X_tst_bias3c_100, X_tst_bias3c_200, X_tst_bias3c_500, X_tst_bias3c_750, X_tst_bias3c_1000\n",
        "y_tst_bias3c = y_tst_bias3c_20, y_tst_bias3c_50, y_tst_bias3c_100, y_tst_bias3c_200, y_tst_bias3c_500, y_tst_bias3c_750, y_tst_bias3c_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTEwd4GEN1YD"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validationc, y_validationc, X_tst_bias3c, y_tst_bias3c):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier5 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier5.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias3\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred4 = classifier5.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred4))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred4))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred4, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred4, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred4))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias3\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred4_test = classifier5.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred4_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred4_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred4_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred4_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred4_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred4_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpdi4VOMN5lJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3flmea6N6MK"
      },
      "source": [
        "**BIAS 4 - NER (CoNLL-2003)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra8EEvELN9yq"
      },
      "source": [
        "**Keyword = 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hjYW0tON_mi"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias4c_20 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_20:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4c_20.append(counter)\n",
        "\n",
        "X_val_biased_4c_20 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias4c_20[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_4c_20.append(temp_num_20)\n",
        "X_val_biased_4c_20 = np.array(X_val_biased_4c_20)\n",
        "\n",
        "X_test_biased_4c_20 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_20 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_20.append(X_bias4c_20[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_4c_20.append(temp_num_20)\n",
        "X_test_biased_4c_20 = np.array(X_test_biased_4c_20)\n",
        "\n",
        "X_val_bias4c_20 = X_val_biased_4c_20\n",
        "y_val_bias4c_20 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4c_20.append(class_made)\n",
        "\n",
        "y_val_bias4c_20 = np.array(y_val_bias4c_20)\n",
        "\n",
        "X_tst_bias4c_20 = X_test_biased_4c_20\n",
        "y_tst_bias4c_20 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4c_20.append(class_made)\n",
        "\n",
        "y_tst_bias4c_20 = np.array(y_tst_bias4c_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6580r7LOERy"
      },
      "source": [
        "**Keyword = 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ku-x7BlODOv"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias4c_50 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_50:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4c_50.append(counter)\n",
        "\n",
        "X_val_biased_4c_50 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias4c_50[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_4c_50.append(temp_num_50)\n",
        "X_val_biased_4c_50 = np.array(X_val_biased_4c_50)\n",
        "\n",
        "X_test_biased_4c_50 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_50 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_50.append(X_bias4c_50[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_4c_50.append(temp_num_50)\n",
        "X_test_biased_4c_50 = np.array(X_test_biased_4c_50)\n",
        "\n",
        "X_val_bias4c_50 = X_val_biased_4c_50\n",
        "y_val_bias4c_50 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4c_50.append(class_made)\n",
        "\n",
        "y_val_bias4c_50 = np.array(y_val_bias4c_50)\n",
        "\n",
        "X_tst_bias4c_50 = X_test_biased_4c_50\n",
        "y_tst_bias4c_50 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4c_50.append(class_made)\n",
        "\n",
        "y_tst_bias4c_50 = np.array(y_tst_bias4c_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bEoa1bOHk-"
      },
      "source": [
        "**Keyword = 100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNNgVLaeOI3r"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias4c_100 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_100:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4c_100.append(counter)\n",
        "\n",
        "X_val_biased_4c_100 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias4c_100[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_4c_100.append(temp_num_100)\n",
        "X_val_biased_4c_100 = np.array(X_val_biased_4c_100)\n",
        "\n",
        "X_test_biased_4c_100 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_100 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_100.append(X_bias4c_100[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_4c_100.append(temp_num_100)\n",
        "X_test_biased_4c_100 = np.array(X_test_biased_4c_100)\n",
        "\n",
        "X_val_bias4c_100 = X_val_biased_4c_100\n",
        "y_val_bias4c_100 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4c_100.append(class_made)\n",
        "\n",
        "y_val_bias4c_100 = np.array(y_val_bias4c_100)\n",
        "\n",
        "X_tst_bias4c_100 = X_test_biased_4c_100\n",
        "y_tst_bias4c_100 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4c_100.append(class_made)\n",
        "\n",
        "y_tst_bias4c_100 = np.array(y_tst_bias4c_100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlMfDA0POK6B"
      },
      "source": [
        "**Keyword = 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TANwRiiaOMXj"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias4c_200 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_200:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4c_200.append(counter)\n",
        "\n",
        "X_val_biased_4c_200 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias4c_200[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_4c_200.append(temp_num_200)\n",
        "X_val_biased_4c_200 = np.array(X_val_biased_4c_200)\n",
        "\n",
        "X_test_biased_4c_200 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_200 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_200.append(X_bias4c_200[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_4c_200.append(temp_num_200)\n",
        "X_test_biased_4c_200 = np.array(X_test_biased_4c_200)\n",
        "\n",
        "X_val_bias4c_200 = X_val_biased_4c_200\n",
        "y_val_bias4c_200 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4c_200.append(class_made)\n",
        "\n",
        "y_val_bias4c_200 = np.array(y_val_bias4c_200)\n",
        "\n",
        "X_tst_bias4c_200 = X_test_biased_4c_200\n",
        "y_tst_bias4c_200 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4c_200.append(class_made)\n",
        "\n",
        "y_tst_bias4c_200 = np.array(y_tst_bias4c_200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-6CKLuHOOM9"
      },
      "source": [
        "**Keyword = 500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnWXu-rsOPnq"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias4c_500 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_500:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4c_500.append(counter)\n",
        "\n",
        "X_val_biased_4c_500 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias4c_500[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_4c_500.append(temp_num_500)\n",
        "X_val_biased_4c_500 = np.array(X_val_biased_4c_500)\n",
        "\n",
        "X_test_biased_4c_500 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_500 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_500.append(X_bias4c_500[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_4c_500.append(temp_num_500)\n",
        "X_test_biased_4c_500 = np.array(X_test_biased_4c_500)\n",
        "\n",
        "X_val_bias4c_500 = X_val_biased_4c_500\n",
        "y_val_bias4c_500 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4c_500.append(class_made)\n",
        "\n",
        "y_val_bias4c_500 = np.array(y_val_bias4c_500)\n",
        "\n",
        "X_tst_bias4c_500 = X_test_biased_4c_500\n",
        "y_tst_bias4c_500 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4c_500.append(class_made)\n",
        "\n",
        "y_tst_bias4c_500 = np.array(y_tst_bias4c_500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxkSfKZFORiJ"
      },
      "source": [
        "**Keyword = 750**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvFeMpU3OTKv"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias4c_750 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_750:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4c_750.append(counter)\n",
        "\n",
        "X_val_biased_4c_750 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias4c_750[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_4c_750.append(temp_num_750)\n",
        "X_val_biased_4c_750 = np.array(X_val_biased_4c_750)\n",
        "\n",
        "X_test_biased_4c_750 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_750 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_750.append(X_bias4c_750[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_4c_750.append(temp_num_750)\n",
        "X_test_biased_4c_750 = np.array(X_test_biased_4c_750)\n",
        "\n",
        "X_val_bias4c_750 = X_val_biased_4c_750\n",
        "y_val_bias4c_750 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4c_750.append(class_made)\n",
        "\n",
        "y_val_bias4c_750 = np.array(y_val_bias4c_750)\n",
        "\n",
        "X_tst_bias4c_750 = X_test_biased_4c_750\n",
        "y_tst_bias4c_750 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4c_750.append(class_made)\n",
        "\n",
        "y_tst_bias4c_750 = np.array(y_tst_bias4c_750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJf9WZ3VOVuU"
      },
      "source": [
        "**Keyword = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ju_pOnHOXFo"
      },
      "source": [
        "# initialise and train the model\n",
        "model2 = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "\n",
        "# create biases in the same fashion as before\n",
        "X_bias4c_1000 = []\n",
        "for w1 in tf_idf_feature_names_training:\n",
        "        counter = 0\n",
        "        averager = 0\n",
        "        for w2 in ner_keyword_list2_1000:\n",
        "            try:\n",
        "                counter += model2.similarity(w1, w2)\n",
        "                averager += 1\n",
        "            except:\n",
        "                averager -= 1\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            counter = counter/averager\n",
        "        except:\n",
        "            counter = counter\n",
        "        if counter > 0:\n",
        "            counter += 1\n",
        "        else:\n",
        "            counter = counter\n",
        "        X_bias4c_1000.append(counter)\n",
        "\n",
        "X_val_biased_4c_1000 = []\n",
        "for i,matrixrow in enumerate(X_val):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias4c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_val_biased_4c_1000.append(temp_num_1000)\n",
        "X_val_biased_4c_1000 = np.array(X_val_biased_4c_1000)\n",
        "\n",
        "X_test_biased_4c_1000 = []\n",
        "for i,matrixrow in enumerate(X_tst):\n",
        "    temp_num_1000 = []\n",
        "    for j, item in enumerate(matrixrow):\n",
        "\n",
        "        temp_num_1000.append(X_bias4c_1000[j]*(item*0.8))\n",
        "\n",
        "    X_test_biased_4c_1000.append(temp_num_1000)\n",
        "X_test_biased_4c_1000 = np.array(X_test_biased_4c_1000)\n",
        "\n",
        "X_val_bias4c_1000 = X_val_biased_4c_1000\n",
        "y_val_bias4c_1000 = []\n",
        "for document in y_trainingdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_val_bias4c_1000.append(class_made)\n",
        "\n",
        "y_val_bias4c_1000 = np.array(y_val_bias4c_1000)\n",
        "\n",
        "X_tst_bias4c_1000 = X_test_biased_4c_1000\n",
        "y_tst_bias4c_1000 = []\n",
        "for document in y_testdata:\n",
        "\n",
        "    class_made = 0\n",
        "    if document == 1:\n",
        "        class_made = 0\n",
        "    else:\n",
        "        class_made = 1\n",
        "    y_tst_bias4c_1000.append(class_made)\n",
        "\n",
        "y_tst_bias4c_1000 = np.array(y_tst_bias4c_1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMrdEbPnOZBL"
      },
      "source": [
        "## **SVM BIAS 4 NER - Keywords 20, 50, 100, 200, 500, 750, 1000 - Max-feat = 4000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P164BNXIObjD"
      },
      "source": [
        "X_validation = X_val_bias4c_20, X_val_bias4c_50, X_val_bias4c_100, X_val_bias4c_200, X_val_bias4c_500, X_val_bias4c_750, X_val_bias4c_1000\n",
        "y_validation = y_val_bias4c_20, y_val_bias4c_50, y_val_bias4c_100, y_val_bias4c_200, y_val_bias4c_500, y_val_bias4c_750, y_val_bias4c_1000\n",
        "\n",
        "X_tst_bias4 = X_tst_bias4c_20, X_tst_bias4c_50, X_tst_bias4c_100, X_tst_bias4c_200, X_tst_bias4c_500, X_tst_bias4c_750, X_tst_bias4c_1000\n",
        "y_tst_bias4 = y_tst_bias4c_20, y_tst_bias4c_50, y_tst_bias4c_100, y_tst_bias4c_200, y_tst_bias4c_500, y_tst_bias4c_750, y_tst_bias4c_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_RVZ5cuOdho"
      },
      "source": [
        "# train test split for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "best_performing_fscore, best_performing_c, best_performing_key = 0, None, None\n",
        "fscore = 0\n",
        "count = 0\n",
        "\n",
        "for x, y, z, a in zip(X_validation, y_validation, X_tst_bias4, y_tst_bias4):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "  for c in SVM_C:\n",
        "    classifier6 = svm.SVC(C=c, kernel='linear', degree=7, gamma='auto', random_state=0)\n",
        "    classifier6.fit(X_train, y_train)\n",
        "\n",
        "# validation\n",
        "    print(\"SVM validation bias4\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred5 = classifier6.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred5))\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred5))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(y_test, y_pred5, average='macro'))\n",
        "    print(precision_recall_fscore_support(y_test, y_pred5, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(y_test, y_pred5))\n",
        "\n",
        "# testing\n",
        "\n",
        "    #for z, a in X_tst_bias1, y_tst_bias1:\n",
        "    print(\"SVM test bias4\", \"SVM_C = \", c, \"number = \", count)\n",
        "    y_pred5_test = classifier6.predict(z)\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    print(confusion_matrix(a, y_pred5_test))\n",
        "    print()\n",
        "    print(classification_report(a, y_pred5_test))\n",
        "    print()\n",
        "    print(precision_recall_fscore_support(a, y_pred5_test, average='macro'))\n",
        "    print(precision_recall_fscore_support(a, y_pred5_test, average='macro')[2])\n",
        "    fscore = (precision_recall_fscore_support(a, y_pred5_test, average='macro')[2])\n",
        "    print()\n",
        "    print(accuracy_score(a, y_pred5_test))\n",
        "\n",
        "    if best_performing_fscore < fscore: \n",
        "      best_performing_fscore, best_performing_c, best_performing_key = fscore, c, count\n",
        "    count+= 1\n",
        "\n",
        "print()\n",
        "print(\"the best performing C parameter is: \", best_performing_c, \"\\n\", \"the best performing keyword number is: \", best_performing_key, \"\\n\", \n",
        "      \"with an f1-score of :\", best_performing_fscore, \"\\n\", \"for max_feat = 4000\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}